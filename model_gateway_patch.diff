*** Begin Patch
*** Update File: llmhive/src/llmhive/app/services/base.py
@@
 class LLMProvider(Protocol):
@@
     async def improve(
         self,
         subject: str,
         *,
         previous_answer: str,
         critiques: list[str],
         model: str,
     ) -> LLMResult:
         """Return an improved answer given critiques."""
+
+    # -------------------------------------------------------------------------
+    # Optional provider extensions
+    #
+    # A model provider may expose multiple model identifiers.  For example,
+    # OpenAI exposes "gpt-3.5-turbo", "gpt-4", etc.  Anthropic might expose
+    # "claude-3-sonnet", "claude-3-haiku", and so on.  To allow the caller to
+    # enumerate which model strings are valid for a provider, implement
+    # `list_models()` and return a list of supported model names.  Providers
+    # that don't override this method should raise NotImplementedError.
+    def list_models(self) -> list[str]:  # pragma: no cover - default method
+        """
+        Return a list of model identifiers supported by this provider.
+
+        Providers exposing multiple models should override this method.
+        """
+        raise NotImplementedError
+
+    async def stream_response(self, prompt: str, *, model: str):  # pragma: no cover - default method
+        """
+        Yield partial response chunks for streaming support.
+
+        Providers that support streaming responses should override this method and
+        return an asynchronous iterator (e.g. an async generator) yielding
+        strings containing incremental parts of the response.  If a provider
+        does not support streaming, it may raise NotImplementedError.
+        """
+        raise NotImplementedError
*** End Patch
*** End Patch
*** Begin Patch
*** Update File: llmhive/src/llmhive/app/services/openai_provider.py
@@ class OpenAIProvider(LLMProvider):
     async def improve(
         self,
         subject: str,
         *,
         previous_answer: str,
         critiques: list[str],
         model: str,
     ) -> LLMResult:
@@
         return await self._chat(messages, model=model)
+
+    def list_models(self) -> list[str]:
+        """Return the list of default model identifiers for the OpenAI provider."""
+        # Expose the configured default models to callers.  These values come
+        # from the application settings and can be overridden via the
+        # DEFAULT_MODELS environment variable.
+        return settings.default_models
+
+    async def stream_response(self, prompt: str, *, model: str):
+        """Stream a chat completion response token by token."""
+        messages = [
+            {"role": "system", "content": "You are an expert assistant working in a collaborative AI team."},
+            {"role": "user", "content": prompt},
+        ]
+        try:
+            stream = await self.client.chat.completions.create(
+                model=model,
+                messages=messages,
+                temperature=0.6,
+                stream=True,
+            )
+            async for part in stream:
+                # Each part is a ChatCompletionChunk; yield the delta content if present.
+                choice = part.choices[0]
+                delta = getattr(choice, "delta", None)
+                if delta and delta.content:
+                    yield delta.content
+        except Exception as exc:  # pragma: no cover - network errors
+            # Propagate provider errors via the configured exception class
+            raise ProviderNotConfiguredError(str(exc)) from exc
*** End Patch
*** End Patch
*** Begin Patch
*** Update File: llmhive/src/llmhive/app/services/stub_provider.py
@@ class StubProvider(LLMProvider):
     async def improve(
         self,
         subject: str,
         *,
         previous_answer: str,
         critiques: List[str],
         model: str,
     ) -> LLMResult:
@@
         content = f"Improved by {model}: {previous_answer} (considering: {critique_text})"
         return LLMResult(content=content[:1500], model=model)
+
+    def list_models(self) -> list[str]:
+        """Return a list containing the name of this stub model."""
+        return ["stub"]
+
+    async def stream_response(self, prompt: str, *, model: str):
+        """Yield the full response as a single chunk for streaming support."""
+        result = await self.complete(prompt, model=model)
+        yield result.content
*** End Patch
*** End Patch
*** Begin Patch
*** Update File: llmhive/src/llmhive/app/services/__init__.py
@@
-from .openai_provider import OpenAIProvider
-from .stub_provider import StubProvider
+from .openai_provider import OpenAIProvider
+from .stub_provider import StubProvider
+from .anthropic_provider import AnthropicProvider
+from .grok_provider import GrokProvider
+from .gemini_provider import GeminiProvider
+from .deepseek_provider import DeepSeekProvider
+from .manus_provider import ManusProvider
@@
 __all__ = [
     "LLMProvider",
     "LLMResult",
     "ProviderError",
     "ProviderNotConfiguredError",
     "OpenAIProvider",
     "StubProvider",
+    "AnthropicProvider",
+    "GrokProvider",
+    "GeminiProvider",
+    "DeepSeekProvider",
+    "ManusProvider",
 ]
*** End Patch
*** End Patch
*** Begin Patch
*** Add File: llmhive/src/llmhive/app/services/anthropic_provider.py
+"""Anthropic provider implementation (placeholder).
+
+This provider wraps the existing OpenAI provider as a fallback until a
+dedicated Anthropic client can be integrated.  It exposes a minimal API
+compatible with the LLMProvider protocol and forwards all calls to the
+OpenAI provider using a configurable model name.
+"""
+
+from __future__ import annotations
+
+from typing import List
+
+from ..config import settings
+from .base import LLMProvider, LLMResult, ProviderNotConfiguredError
+from .openai_provider import OpenAIProvider
+
+
+class AnthropicProvider(LLMProvider):
+    """Interact with Anthropic Claude models via a delegated provider."""
+
+    def __init__(self, api_key: str | None = None, timeout: float | None = None) -> None:
+        # Until an official Anthropic client is integrated, fall back to
+        # OpenAIProvider.  In future, replace this with calls to the
+        # anthropic-python SDK (anthropic.Client) using settings.anthropic_api_key.
+        self.provider = OpenAIProvider(api_key=api_key, timeout=timeout)
+        # Default model for Anthropic; can be overridden by passing ``model``
+        # explicitly when calling provider methods.
+        self.models: List[str] = ["claude-3-sonnet-20240229"]
+
+    def list_models(self) -> list[str]:
+        return self.models
+
+    async def complete(self, prompt: str, *, model: str) -> LLMResult:
+        return await self.provider.complete(prompt, model=model)
+
+    async def critique(
+        self, subject: str, *, target_answer: str, author: str, model: str
+    ) -> LLMResult:
+        return await self.provider.critique(subject, target_answer=target_answer, author=author, model=model)
+
+    async def improve(
+        self,
+        subject: str,
+        *,
+        previous_answer: str,
+        critiques: list[str],
+        model: str,
+    ) -> LLMResult:
+        return await self.provider.improve(subject, previous_answer=previous_answer, critiques=critiques, model=model)
+
+    async def stream_response(self, prompt: str, *, model: str):
+        async for chunk in self.provider.stream_response(prompt, model=model):
+            yield chunk
*** End Patch
*** End Patch
*** Begin Patch
*** Add File: llmhive/src/llmhive/app/services/grok_provider.py
+"""Grok provider implementation (placeholder).
+
+This provider delegates all operations to the OpenAI provider until a
+dedicated xAI Grok client can be integrated.  Grok is reported to be
+compatible with the OpenAI API when using a custom base URL and API key.
+Here we simply forward calls to OpenAIProvider with a configurable model
+identifier.
+"""
+
+from __future__ import annotations
+
+from typing import List
+
+from ..config import settings
+from .base import LLMProvider, LLMResult
+from .openai_provider import OpenAIProvider
+
+
+class GrokProvider(LLMProvider):
+    """Interact with xAI's Grok models via a delegated provider."""
+
+    def __init__(self, api_key: str | None = None, timeout: float | None = None) -> None:
+        # Future enhancement: configure base_url and api_key for xAI Grok
+        self.provider = OpenAIProvider(api_key=api_key, timeout=timeout)
+        self.models: List[str] = ["grok-1"]
+
+    def list_models(self) -> list[str]:
+        return self.models
+
+    async def complete(self, prompt: str, *, model: str) -> LLMResult:
+        return await self.provider.complete(prompt, model=model)
+
+    async def critique(
+        self, subject: str, *, target_answer: str, author: str, model: str
+    ) -> LLMResult:
+        return await self.provider.critique(subject, target_answer=target_answer, author=author, model=model)
+
+    async def improve(
+        self,
+        subject: str,
+        *,
+        previous_answer: str,
+        critiques: list[str],
+        model: str,
+    ) -> LLMResult:
+        return await self.provider.improve(subject, previous_answer=previous_answer, critiques=critiques, model=model)
+
+    async def stream_response(self, prompt: str, *, model: str):
+        async for chunk in self.provider.stream_response(prompt, model=model):
+            yield chunk
*** End Patch
*** End Patch
*** Begin Patch
*** Add File: llmhive/src/llmhive/app/services/gemini_provider.py
+"""Gemini provider implementation (placeholder).
+
+This provider delegates operations to the OpenAI provider as a temporary
+measure.  Google’s Gemini models can often be accessed via an OpenAI-
+compatible API or via the google-generative-ai SDK.  To keep the
+interface consistent, we forward requests to OpenAIProvider with a
+Gemini-specific model identifier.  Replace this with a real Gemini
+integration when available.
+"""
+
+from __future__ import annotations
+
+from typing import List
+
+from ..config import settings
+from .base import LLMProvider, LLMResult
+from .openai_provider import OpenAIProvider
+
+
+class GeminiProvider(LLMProvider):
+    """Interact with Google’s Gemini models via a delegated provider."""
+
+    def __init__(self, api_key: str | None = None, timeout: float | None = None) -> None:
+        # Future enhancement: use google.generativeai.Client with settings.gemini_api_key
+        self.provider = OpenAIProvider(api_key=api_key, timeout=timeout)
+        self.models: List[str] = ["gemini-pro"]
+
+    def list_models(self) -> list[str]:
+        return self.models
+
+    async def complete(self, prompt: str, *, model: str) -> LLMResult:
+        return await self.provider.complete(prompt, model=model)
+
+    async def critique(
+        self, subject: str, *, target_answer: str, author: str, model: str
+    ) -> LLMResult:
+        return await self.provider.critique(subject, target_answer=target_answer, author=author, model=model)
+
+    async def improve(
+        self,
+        subject: str,
+        *,
+        previous_answer: str,
+        critiques: list[str],
+        model: str,
+    ) -> LLMResult:
+        return await self.provider.improve(subject, previous_answer=previous_answer, critiques=critiques, model=model)
+
+    async def stream_response(self, prompt: str, *, model: str):
+        async for chunk in self.provider.stream_response(prompt, model=model):
+            yield chunk
*** End Patch
*** End Patch
*** Begin Patch
*** Add File: llmhive/src/llmhive/app/services/deepseek_provider.py
+"""DeepSeek provider implementation (placeholder).
+
+DeepSeek exposes OpenAI-compatible endpoints.  Until direct integration
+is available, we forward all operations to the OpenAI provider and use
+a DeepSeek-specific model identifier.  Replace this with a direct
+DeepSeek API client when one becomes available.
+"""
+
+from __future__ import annotations
+
+from typing import List
+
+from ..config import settings
+from .base import LLMProvider, LLMResult
+from .openai_provider import OpenAIProvider
+
+
+class DeepSeekProvider(LLMProvider):
+    """Interact with DeepSeek models via a delegated provider."""
+
+    def __init__(self, api_key: str | None = None, timeout: float | None = None) -> None:
+        # Future enhancement: set base_url to DeepSeek endpoint and pass api_key
+        self.provider = OpenAIProvider(api_key=api_key, timeout=timeout)
+        self.models: List[str] = ["deepseek-chat"]
+
+    def list_models(self) -> list[str]:
+        return self.models
+
+    async def complete(self, prompt: str, *, model: str) -> LLMResult:
+        return await self.provider.complete(prompt, model=model)
+
+    async def critique(
+        self, subject: str, *, target_answer: str, author: str, model: str
+    ) -> LLMResult:
+        return await self.provider.critique(subject, target_answer=target_answer, author=author, model=model)
+
+    async def improve(
+        self,
+        subject: str,
+        *,
+        previous_answer: str,
+        critiques: list[str],
+        model: str,
+    ) -> LLMResult:
+        return await self.provider.improve(subject, previous_answer=previous_answer, critiques=critiques, model=model)
+
+    async def stream_response(self, prompt: str, *, model: str):
+        async for chunk in self.provider.stream_response(prompt, model=model):
+            yield chunk
*** End Patch
*** End Patch
*** Begin Patch
*** Add File: llmhive/src/llmhive/app/services/manus_provider.py
+"""Manus provider implementation (placeholder).
+
+Manus may provide autonomous agent capabilities through its own API.  Until
+a first-class Manus client can be integrated, this provider forwards
+requests to the OpenAI provider with a Manus-specific model name.  When
+Manus publishes a stable API, replace these implementations with real
+Manus API calls.
+"""
+
+from __future__ import annotations
+
+from typing import List
+
+from ..config import settings
+from .base import LLMProvider, LLMResult
+from .openai_provider import OpenAIProvider
+
+
+class ManusProvider(LLMProvider):
+    """Interact with Manus models via a delegated provider."""
+
+    def __init__(self, api_key: str | None = None, timeout: float | None = None) -> None:
+        # Future enhancement: implement Manus agent integration
+        self.provider = OpenAIProvider(api_key=api_key, timeout=timeout)
+        self.models: List[str] = ["manus"]
+
+    def list_models(self) -> list[str]:
+        return self.models
+
+    async def complete(self, prompt: str, *, model: str) -> LLMResult:
+        return await self.provider.complete(prompt, model=model)
+
+    async def critique(
+        self, subject: str, *, target_answer: str, author: str, model: str
+    ) -> LLMResult:
+        return await self.provider.critique(subject, target_answer=target_answer, author=author, model=model)
+
+    async def improve(
+        self,
+        subject: str,
+        *,
+        previous_answer: str,
+        critiques: list[str],
+        model: str,
+    ) -> LLMResult:
+        return await self.provider.improve(subject, previous_answer=previous_answer, critiques=critiques, model=model)
+
+    async def stream_response(self, prompt: str, *, model: str):
+        async for chunk in self.provider.stream_response(prompt, model=model):
+            yield chunk
*** End Patch
*** End Patch
