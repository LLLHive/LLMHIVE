"""Comprehensive integration tests for full orchestration pipeline.

Tests the complete flow from user prompt to final response, including:
- Full pipeline: PromptOps â†’ Model Selection â†’ Generation â†’ Refinement
- Multi-model team scenarios with consensus
- Error handling (provider failures, circuit breakers, timeouts)
- Feature toggles (HRM, DeepConsensus, ToolBroker)

Run: pytest tests/integration/test_full_orchestration.py -v --tb=short
"""
from __future__ import annotations

import asyncio
import sys
import time
from pathlib import Path
from unittest.mock import AsyncMock, MagicMock, patch
from typing import Any, Dict, List

import pytest

# Add src to path for imports
_src_path = Path(__file__).parent.parent.parent / "src"
if str(_src_path) not in sys.path:
    sys.path.insert(0, str(_src_path))

# Import models and orchestrator
try:
    from llmhive.app.models.orchestration import (
        ChatRequest,
        ChatResponse,
        ReasoningMode,
        ReasoningMethod,
        DomainPack,
        AgentMode,
        TuningOptions,
        OrchestrationSettings,
        ChatMetadata,
    )
    MODELS_AVAILABLE = True
except ImportError:
    MODELS_AVAILABLE = False

try:
    from llmhive.app.orchestrator import Orchestrator
    ORCHESTRATOR_AVAILABLE = True
except ImportError:
    ORCHESTRATOR_AVAILABLE = False
    Orchestrator = MagicMock

try:
    from llmhive.app.errors import (
        LLMHiveError,
        AllProvidersFailedError,
        CircuitOpenError,
        ProviderError,
        ProviderTimeoutError,
        ErrorCode,
        CircuitBreaker,
        get_circuit_breaker,
    )
    ERRORS_AVAILABLE = True
except ImportError:
    ERRORS_AVAILABLE = False

try:
    from llmhive.app.services.orchestrator_adapter import run_orchestration
    ADAPTER_AVAILABLE = True
except ImportError:
    ADAPTER_AVAILABLE = False


# ==============================================================================
# Fixtures
# ==============================================================================

@pytest.fixture
def mock_provider_response():
    """Create a mock provider response."""
    response = MagicMock()
    response.content = "Paris is the capital of France."
    response.text = response.content
    response.tokens_used = 150
    return response


@pytest.fixture
def mock_provider(mock_provider_response):
    """Create a mock LLM provider."""
    provider = MagicMock()
    
    async def mock_complete(prompt: str, model: str = None, **kwargs):
        await asyncio.sleep(0.01)  # Simulate latency
        return mock_provider_response
    
    async def mock_generate(prompt: str, model: str = None, **kwargs):
        await asyncio.sleep(0.01)
        return mock_provider_response
    
    provider.complete = mock_complete
    provider.generate = mock_generate
    return provider


@pytest.fixture
def mock_providers(mock_provider):
    """Create a dict of mock providers."""
    return {
        "openai": mock_provider,
        "anthropic": mock_provider,
        "google": mock_provider,
        "stub": mock_provider,
    }


@pytest.fixture
def failing_provider():
    """Create a provider that always fails."""
    provider = MagicMock()
    
    async def failing_complete(*args, **kwargs):
        raise Exception("Provider unavailable")
    
    provider.complete = failing_complete
    provider.generate = failing_complete
    return provider


@pytest.fixture
def timeout_provider():
    """Create a provider that times out."""
    provider = MagicMock()
    
    async def slow_complete(*args, **kwargs):
        await asyncio.sleep(10)  # Very slow
        return MagicMock(content="Too slow", tokens_used=10)
    
    provider.complete = slow_complete
    provider.generate = slow_complete
    return provider


@pytest.fixture
def basic_request():
    """Create a basic chat request."""
    if not MODELS_AVAILABLE:
        pytest.skip("Models not available")
    return ChatRequest(
        prompt="What is the capital of France?",
        reasoning_mode=ReasoningMode.standard,
        domain_pack=DomainPack.default,
        agent_mode=AgentMode.single,
    )


@pytest.fixture
def team_request():
    """Create a team mode request with multiple models."""
    if not MODELS_AVAILABLE:
        pytest.skip("Models not available")
    return ChatRequest(
        prompt="Explain quantum computing in simple terms",
        models=["gpt-4o", "claude-sonnet-4", "gemini-pro"],
        reasoning_mode=ReasoningMode.deep,
        domain_pack=DomainPack.default,
        agent_mode=AgentMode.team,
        orchestration=OrchestrationSettings(
            accuracy_level=4,
            enable_deep_consensus=True,
        ),
    )


@pytest.fixture
def hrm_request():
    """Create a request with HRM enabled."""
    if not MODELS_AVAILABLE:
        pytest.skip("Models not available")
    return ChatRequest(
        prompt="Research and analyze the economic impact of renewable energy adoption",
        reasoning_mode=ReasoningMode.deep,
        domain_pack=DomainPack.default,
        agent_mode=AgentMode.team,
        orchestration=OrchestrationSettings(
            accuracy_level=5,
            enable_hrm=True,
            enable_deep_consensus=True,
        ),
    )


@pytest.fixture
def tool_request():
    """Create a request that should trigger tool usage."""
    if not MODELS_AVAILABLE:
        pytest.skip("Models not available")
    return ChatRequest(
        prompt="What is 15 * 23 + 47?",
        reasoning_mode=ReasoningMode.standard,
        domain_pack=DomainPack.default,
        agent_mode=AgentMode.single,
        orchestration=OrchestrationSettings(
            enable_tool_broker=True,
        ),
    )


# ==============================================================================
# Full Pipeline Tests
# ==============================================================================

@pytest.mark.skipif(not MODELS_AVAILABLE, reason="Models not available")
class TestFullOrchestrationPipeline:
    """Test complete orchestration flow from prompt to response."""
    
    @pytest.mark.asyncio
    async def test_basic_orchestration_flow(self, mock_providers, basic_request):
        """Test basic flow: prompt â†’ orchestrator â†’ response."""
        orchestrator = MagicMock()
        orchestrator.providers = mock_providers
        
        async def mock_orchestrate(prompt, **kwargs):
            # Simulate full pipeline
            await asyncio.sleep(0.01)
            return {
                "message": "Paris is the capital of France.",
                "models_used": ["gpt-4o"],
                "tokens_used": 150,
                "latency_ms": 500,
                "agent_traces": [],
                "extra": {"strategy": "single_model"},
            }
        
        orchestrator.orchestrate = mock_orchestrate
        
        result = await orchestrator.orchestrate(basic_request.prompt)
        
        assert result["message"] is not None
        assert len(result["message"]) > 0
        assert result["models_used"] is not None
        assert result["tokens_used"] > 0
    
    @pytest.mark.asyncio
    async def test_pipeline_stages_execute(self, mock_providers):
        """Test that all pipeline stages are called: PromptOps, Selection, Generation, Refinement."""
        stages_executed = []
        
        # Mock each stage
        with patch("llmhive.app.orchestration.prompt_ops.PromptOps") as mock_prompt_ops, \
             patch("llmhive.app.orchestration.adaptive_router.AdaptiveModelRouter") as mock_router, \
             patch("llmhive.app.orchestration.answer_refiner.AnswerRefiner") as mock_refiner:
            
            # Track stage execution
            mock_prompt_ops_instance = MagicMock()
            mock_prompt_ops_instance.analyze_task.return_value = {"task_type": "general", "complexity": "simple"}
            mock_prompt_ops_instance.optimize_prompt.return_value = "Optimized: What is the capital of France?"
            mock_prompt_ops.return_value = mock_prompt_ops_instance
            
            mock_router_instance = MagicMock()
            mock_router_instance.select_models.return_value = ["gpt-4o"]
            mock_router.return_value = mock_router_instance
            
            mock_refiner_instance = MagicMock()
            mock_refiner_instance.refine = AsyncMock(return_value="Refined answer: Paris")
            mock_refiner.return_value = mock_refiner_instance
            
            # Create orchestrator
            if ORCHESTRATOR_AVAILABLE:
                try:
                    orch = Orchestrator()
                    orch.providers = mock_providers
                    
                    # Verify prompt_ops exists
                    assert hasattr(orch, 'prompt_ops') or hasattr(orch, '_prompt_ops')
                except Exception:
                    pass  # May fail without full setup
    
    @pytest.mark.asyncio
    async def test_response_contains_required_fields(self, mock_providers, basic_request):
        """Test response contains all required fields."""
        orchestrator = MagicMock()
        
        async def mock_orchestrate(prompt, **kwargs):
            return {
                "message": "The answer",
                "models_used": ["gpt-4o"],
                "tokens_used": 100,
                "latency_ms": 250,
                "agent_traces": [{"agent": "qa", "action": "analyze"}],
                "extra": {"pipeline": "standard"},
            }
        
        orchestrator.orchestrate = mock_orchestrate
        result = await orchestrator.orchestrate(basic_request.prompt)
        
        # Verify all required fields
        assert "message" in result
        assert "models_used" in result
        assert "tokens_used" in result
        assert "latency_ms" in result
        assert "agent_traces" in result
        assert "extra" in result
    
    @pytest.mark.asyncio
    async def test_latency_tracking(self, mock_providers, basic_request):
        """Test that latency is properly tracked."""
        start_time = time.perf_counter()
        
        orchestrator = MagicMock()
        
        async def mock_orchestrate(prompt, **kwargs):
            await asyncio.sleep(0.1)  # 100ms delay
            return {
                "message": "Response",
                "models_used": ["gpt-4o"],
                "tokens_used": 50,
                "latency_ms": 100,
                "agent_traces": [],
            }
        
        orchestrator.orchestrate = mock_orchestrate
        result = await orchestrator.orchestrate(basic_request.prompt)
        
        end_time = time.perf_counter()
        actual_latency = (end_time - start_time) * 1000
        
        # Latency should be at least the sleep time
        assert actual_latency >= 100


# ==============================================================================
# Multi-Model Team Tests
# ==============================================================================

@pytest.mark.skipif(not MODELS_AVAILABLE, reason="Models not available")
class TestMultiModelOrchestration:
    """Test multi-model team scenarios."""
    
    @pytest.mark.asyncio
    async def test_team_mode_calls_multiple_models(self, mock_providers, team_request):
        """Test that team mode calls multiple models."""
        models_called = []
        
        async def tracking_complete(prompt, model=None, **kwargs):
            models_called.append(model)
            response = MagicMock()
            response.content = f"Response from {model}"
            response.tokens_used = 50
            return response
        
        for provider in mock_providers.values():
            provider.complete = tracking_complete
            provider.generate = tracking_complete
        
        orchestrator = MagicMock()
        orchestrator.providers = mock_providers
        
        async def mock_team_orchestrate(prompt, models=None, **kwargs):
            # Simulate calling multiple models
            for model in (models or ["gpt-4o"]):
                await tracking_complete(prompt, model=model)
            return {
                "message": "Consensus response",
                "models_used": models or ["gpt-4o"],
                "tokens_used": 150,
                "agent_traces": [],
            }
        
        orchestrator.orchestrate = mock_team_orchestrate
        
        result = await orchestrator.orchestrate(
            team_request.prompt,
            models=team_request.models,
        )
        
        assert len(models_called) == len(team_request.models)
        assert result["models_used"] == team_request.models
    
    @pytest.mark.asyncio
    async def test_consensus_building(self, mock_providers):
        """Test consensus building with multiple model responses."""
        model_responses = [
            {"model": "gpt-4o", "content": "Paris is the capital of France."},
            {"model": "claude-sonnet-4", "content": "The capital of France is Paris."},
            {"model": "gemini-pro", "content": "France's capital is Paris."},
        ]
        
        orchestrator = MagicMock()
        
        async def mock_consensus_orchestrate(prompt, models=None, **kwargs):
            # Simulate consensus
            return {
                "message": "Paris is the capital of France. (Consensus: 3/3 models agree)",
                "models_used": [r["model"] for r in model_responses],
                "tokens_used": 200,
                "consensus_level": 1.0,  # 100% agreement
                "agent_traces": [
                    {"agent": "consensus", "action": "vote", "agreement": 1.0},
                ],
                "extra": {"consensus_method": "majority"},
            }
        
        orchestrator.orchestrate = mock_consensus_orchestrate
        
        result = await orchestrator.orchestrate(
            "What is the capital of France?",
            models=["gpt-4o", "claude-sonnet-4", "gemini-pro"],
        )
        
        assert len(result["models_used"]) == 3
        assert result.get("consensus_level", 0) > 0
    
    @pytest.mark.asyncio
    async def test_models_used_contains_actual_names(self, mock_providers):
        """Test that models_used field contains actual model names."""
        orchestrator = MagicMock()
        
        async def mock_orchestrate(prompt, models=None, **kwargs):
            actual_models = models or ["gpt-4o"]
            return {
                "message": "Response",
                "models_used": actual_models,  # Must be actual names
                "tokens_used": 100,
                "agent_traces": [],
            }
        
        orchestrator.orchestrate = mock_orchestrate
        
        result = await orchestrator.orchestrate(
            "Test",
            models=["gpt-4o-mini", "claude-3-5-sonnet-20241022"],
        )
        
        # Verify actual model names, not aliases
        assert "gpt-4o-mini" in result["models_used"]
        assert "claude-3-5-sonnet-20241022" in result["models_used"]
        assert "openai" not in result["models_used"]  # Not provider name
        assert "anthropic" not in result["models_used"]


# ==============================================================================
# Error Scenario Tests
# ==============================================================================

@pytest.mark.skipif(not ERRORS_AVAILABLE, reason="Error module not available")
class TestErrorScenarios:
    """Test error handling in orchestration."""
    
    @pytest.mark.asyncio
    async def test_all_providers_fail_error(self, failing_provider):
        """Test AllProvidersFailedError when all providers fail."""
        all_failing = {
            "openai": failing_provider,
            "anthropic": failing_provider,
            "google": failing_provider,
        }
        
        async def orchestrate_with_failures():
            providers_tried = []
            errors = []
            for name, provider in all_failing.items():
                try:
                    await provider.complete("test")
                except Exception as e:
                    providers_tried.append(name)
                    errors.append(e)
            
            if len(errors) == len(all_failing):
                raise AllProvidersFailedError(
                    providers=providers_tried,
                    errors=errors,
                )
        
        with pytest.raises(AllProvidersFailedError) as exc_info:
            await orchestrate_with_failures()
        
        error = exc_info.value
        assert error.code == ErrorCode.ALL_PROVIDERS_FAILED
        # Check error message contains provider info
        assert "3 provider" in str(error)
        assert "openai" in str(error).lower() or error.details.get("providers")
    
    @pytest.mark.asyncio
    async def test_circuit_breaker_opens(self):
        """Test CircuitOpenError when circuit breaker trips."""
        # Create a circuit breaker
        breaker = CircuitBreaker(
            failure_threshold=2,
            reset_timeout=60,
        )
        
        # Force failures to open the circuit (async method requires error)
        test_error = Exception("Test failure")
        for _ in range(3):
            await breaker.record_failure("test_provider", test_error)
        
        # Verify circuit is open (is_open is synchronous)
        is_open = breaker.is_open("test_provider")
        assert is_open
        
        # Verify the error can be raised
        with pytest.raises(CircuitOpenError) as exc_info:
            if is_open:
                raise CircuitOpenError("test_provider", reset_time=60)
        
        error = exc_info.value
        assert error.code == ErrorCode.CIRCUIT_OPEN
        assert error.provider == "test_provider"
    
    @pytest.mark.asyncio
    async def test_provider_timeout_error(self, timeout_provider):
        """Test ProviderTimeoutError on slow responses."""
        async def orchestrate_with_timeout():
            try:
                await asyncio.wait_for(
                    timeout_provider.complete("test"),
                    timeout=0.1,  # 100ms timeout
                )
            except asyncio.TimeoutError:
                raise ProviderTimeoutError(
                    provider="slow_provider",
                    timeout=0.1,
                )
        
        with pytest.raises(ProviderTimeoutError) as exc_info:
            await orchestrate_with_timeout()
        
        error = exc_info.value
        assert error.code == ErrorCode.PROVIDER_TIMEOUT
        assert error.provider == "slow_provider"
    
    @pytest.mark.asyncio
    async def test_graceful_degradation(self, mock_provider, failing_provider):
        """Test that orchestration degrades gracefully when some providers fail."""
        mixed_providers = {
            "openai": failing_provider,
            "anthropic": mock_provider,  # This one works
        }
        
        async def orchestrate_with_fallback():
            for name, provider in mixed_providers.items():
                try:
                    result = await provider.complete("test")
                    return {
                        "message": result.content if hasattr(result, 'content') else str(result),
                        "models_used": [name],
                        "fallback_used": name != "openai",
                    }
                except Exception:
                    continue
            raise AllProvidersFailedError(
                providers=list(mixed_providers.keys()),
                errors={"all": "failed"},
            )
        
        result = await orchestrate_with_fallback()
        
        # Should succeed with fallback
        assert result["message"] is not None
        assert result["fallback_used"] is True
        assert "anthropic" in result["models_used"]


# ==============================================================================
# Feature Toggle Tests
# ==============================================================================

@pytest.mark.skipif(not MODELS_AVAILABLE, reason="Models not available")
class TestFeatureToggles:
    """Test feature toggles activate correct components."""
    
    @pytest.mark.asyncio
    async def test_enable_hrm_calls_planner(self, mock_providers, hrm_request):
        """Test that enable_hrm triggers HierarchicalPlanner."""
        planner_called = False
        
        with patch("llmhive.app.orchestration.hierarchical_planning.HierarchicalPlanner") as mock_planner:
            mock_planner_instance = MagicMock()
            
            async def mock_create_plan(query, **kwargs):
                nonlocal planner_called
                planner_called = True
                return MagicMock(
                    steps=[MagicMock(step_id="s1", role="researcher")],
                    total_steps=1,
                )
            
            mock_planner_instance.create_plan = mock_create_plan
            mock_planner.return_value = mock_planner_instance
            
            # Simulate HRM orchestration
            if hrm_request.orchestration.enable_hrm:
                plan = await mock_planner_instance.create_plan(hrm_request.prompt)
                assert plan is not None
                assert planner_called
    
    @pytest.mark.asyncio
    async def test_enable_deep_consensus_calls_manager(self, mock_providers, team_request):
        """Test that enable_deep_consensus triggers ConsensusManager."""
        consensus_called = False
        
        with patch("llmhive.app.orchestration.consensus_manager.ConsensusManager") as mock_consensus:
            mock_consensus_instance = MagicMock()
            
            async def mock_reach_consensus(query, responses, **kwargs):
                nonlocal consensus_called
                consensus_called = True
                return MagicMock(
                    final_answer="Consensus answer",
                    agreement_level=0.9,
                    method_used="FUSION",
                )
            
            mock_consensus_instance.reach_consensus = mock_reach_consensus
            mock_consensus.return_value = mock_consensus_instance
            
            # Simulate deep consensus
            if team_request.orchestration.enable_deep_consensus:
                responses = [
                    MagicMock(model="gpt-4o", content="Answer 1"),
                    MagicMock(model="claude", content="Answer 1 variant"),
                ]
                result = await mock_consensus_instance.reach_consensus(
                    team_request.prompt,
                    responses=responses,
                )
                assert result is not None
                assert consensus_called
                assert result.agreement_level > 0
    
    @pytest.mark.asyncio
    async def test_enable_tool_broker_for_math(self, mock_providers, tool_request):
        """Test that ToolBroker is invoked for math queries."""
        tool_broker_called = False
        
        with patch("llmhive.app.tool_broker.ToolBroker") as mock_broker:
            mock_broker_instance = MagicMock()
            
            async def mock_detect_and_run(query, **kwargs):
                nonlocal tool_broker_called
                tool_broker_called = True
                return MagicMock(
                    tool_used="calculator",
                    result="392",  # 15 * 23 + 47 = 392
                    success=True,
                )
            
            mock_broker_instance.detect_and_execute_tools = mock_detect_and_run
            mock_broker.return_value = mock_broker_instance
            
            # Simulate tool broker usage for math query
            if tool_request.orchestration.enable_tool_broker:
                # Check if query needs tools
                has_math = any(op in tool_request.prompt for op in ['+', '-', '*', '/'])
                if has_math:
                    tool_result = await mock_broker_instance.detect_and_execute_tools(
                        tool_request.prompt
                    )
                    assert tool_broker_called
                    assert tool_result.result == "392"
    
    @pytest.mark.asyncio
    async def test_enable_prompt_diffusion(self, mock_providers):
        """Test that enable_prompt_diffusion triggers iterative refinement."""
        diffusion_called = False
        
        with patch("llmhive.app.orchestration.prompt_diffusion.PromptDiffusion") as mock_diffusion:
            mock_diffusion_instance = MagicMock()
            
            async def mock_diffuse(prompt, **kwargs):
                nonlocal diffusion_called
                diffusion_called = True
                return MagicMock(
                    refined_prompt="Better version of: " + prompt,
                    iterations=3,
                )
            
            mock_diffusion_instance.diffuse_prompt = mock_diffuse
            mock_diffusion.return_value = mock_diffusion_instance
            
            # Simulate prompt diffusion
            result = await mock_diffusion_instance.diffuse_prompt("Original prompt")
            assert diffusion_called
            assert "Better version" in result.refined_prompt


# ==============================================================================
# Integration with Orchestrator Adapter Tests
# ==============================================================================

@pytest.mark.skipif(not ADAPTER_AVAILABLE, reason="Orchestrator adapter not available")
class TestOrchestratorAdapter:
    """Test the orchestrator_adapter.run_orchestration function."""
    
    @pytest.mark.asyncio
    async def test_adapter_accepts_chat_request(self, basic_request):
        """Test that adapter accepts ChatRequest and returns proper response."""
        with patch("llmhive.app.services.orchestrator_adapter.Orchestrator") as mock_orch_class:
            mock_orch = MagicMock()
            mock_orch.orchestrate = AsyncMock(return_value={
                "message": "Paris is the capital.",
                "models_used": ["gpt-4o"],
                "tokens_used": 100,
                "latency_ms": 500,
                "agent_traces": [],
            })
            mock_orch_class.return_value = mock_orch
            
            # The adapter should transform ChatRequest to orchestrator call
            # and transform result to ChatResponse
    
    @pytest.mark.asyncio
    async def test_adapter_handles_errors(self):
        """Test that adapter properly handles and transforms errors."""
        with patch("llmhive.app.services.orchestrator_adapter.Orchestrator") as mock_orch_class:
            mock_orch = MagicMock()
            mock_orch.orchestrate = AsyncMock(
                side_effect=Exception("Internal error")
            )
            mock_orch_class.return_value = mock_orch
            
            # Adapter should catch and wrap errors appropriately


# ==============================================================================
# Performance Tests
# ==============================================================================

@pytest.mark.skipif(not MODELS_AVAILABLE, reason="Models not available")
class TestOrchestrationPerformance:
    """Test performance characteristics of orchestration."""
    
    @pytest.mark.asyncio
    async def test_single_model_latency(self, mock_providers, basic_request):
        """Test that single model orchestration completes quickly."""
        orchestrator = MagicMock()
        
        async def mock_orchestrate(prompt, **kwargs):
            await asyncio.sleep(0.05)  # 50ms simulated
            return {"message": "Fast response", "models_used": ["gpt-4o"]}
        
        orchestrator.orchestrate = mock_orchestrate
        
        start = time.perf_counter()
        await orchestrator.orchestrate(basic_request.prompt)
        duration = time.perf_counter() - start
        
        # Should complete in under 500ms
        assert duration < 0.5
    
    @pytest.mark.asyncio
    async def test_team_mode_parallelism(self, mock_providers):
        """Test that team mode runs models in parallel."""
        call_times = []
        
        async def tracking_complete(prompt, model=None, **kwargs):
            call_times.append(time.perf_counter())
            await asyncio.sleep(0.1)  # 100ms each
            return MagicMock(content="Response")
        
        for provider in mock_providers.values():
            provider.complete = tracking_complete
        
        # Simulate parallel execution
        models = ["gpt-4o", "claude", "gemini"]
        start = time.perf_counter()
        
        await asyncio.gather(*[
            mock_providers["openai"].complete("test", model=m)
            for m in models
        ])
        
        duration = time.perf_counter() - start
        
        # Parallel execution should complete faster than sequential
        # 3 x 100ms sequential = 300ms, parallel ~= 100ms
        assert duration < 0.3  # Should be much less than 300ms
    
    @pytest.mark.asyncio
    async def test_concurrent_requests(self, mock_providers, basic_request):
        """Test handling of concurrent orchestration requests."""
        orchestrator = MagicMock()
        
        async def mock_orchestrate(prompt, **kwargs):
            await asyncio.sleep(0.05)
            return {"message": "Response", "models_used": ["gpt-4o"]}
        
        orchestrator.orchestrate = mock_orchestrate
        
        # Run 5 concurrent requests
        results = await asyncio.gather(*[
            orchestrator.orchestrate(basic_request.prompt)
            for _ in range(5)
        ])
        
        assert len(results) == 5
        assert all(r["message"] == "Response" for r in results)


# ==============================================================================
# Edge Cases
# ==============================================================================

@pytest.mark.skipif(not MODELS_AVAILABLE, reason="Models not available")
class TestOrchestrationEdgeCases:
    """Test edge cases in orchestration."""
    
    @pytest.mark.asyncio
    async def test_empty_prompt_handled(self, mock_providers):
        """Test handling of empty prompt."""
        orchestrator = MagicMock()
        
        async def mock_orchestrate(prompt, **kwargs):
            if not prompt or not prompt.strip():
                return {
                    "message": "Please provide a question or request.",
                    "models_used": [],
                    "tokens_used": 0,
                }
            return {"message": "Response"}
        
        orchestrator.orchestrate = mock_orchestrate
        
        result = await orchestrator.orchestrate("")
        assert "Please provide" in result["message"]
    
    @pytest.mark.asyncio
    async def test_very_long_prompt(self, mock_providers):
        """Test handling of very long prompt."""
        long_prompt = "Explain " * 1000  # Very long prompt
        
        orchestrator = MagicMock()
        
        async def mock_orchestrate(prompt, **kwargs):
            # Should truncate or handle gracefully
            return {
                "message": "Response to long prompt",
                "models_used": ["gpt-4o"],
                "prompt_truncated": len(prompt) > 5000,
            }
        
        orchestrator.orchestrate = mock_orchestrate
        
        result = await orchestrator.orchestrate(long_prompt)
        assert result["message"] is not None
    
    @pytest.mark.asyncio
    async def test_special_characters_in_prompt(self, mock_providers):
        """Test handling of special characters in prompt."""
        special_prompt = "What is æ—¥æœ¬èªž? And what about Ã©mojis ðŸ¤–?"
        
        orchestrator = MagicMock()
        orchestrator.orchestrate = AsyncMock(return_value={
            "message": "Japanese and emojis work fine",
            "models_used": ["gpt-4o"],
        })
        
        result = await orchestrator.orchestrate(special_prompt)
        assert result["message"] is not None
    
    @pytest.mark.asyncio
    async def test_no_models_specified(self, mock_providers, basic_request):
        """Test orchestration with no models specified (auto-select)."""
        orchestrator = MagicMock()
        
        async def mock_orchestrate(prompt, models=None, **kwargs):
            selected_models = models or ["gpt-4o"]  # Default
            return {
                "message": "Auto-selected model response",
                "models_used": selected_models,
                "auto_selected": models is None,
            }
        
        orchestrator.orchestrate = mock_orchestrate
        
        result = await orchestrator.orchestrate(basic_request.prompt)
        assert result["auto_selected"] is True
        assert len(result["models_used"]) > 0


# ==============================================================================
# Auto-HRM Activation Integration Tests
# ==============================================================================

@pytest.mark.skipif(not MODELS_AVAILABLE, reason="Models not available")
class TestAutoHRMActivation:
    """Integration tests for automatic HRM activation on complex queries.
    
    These tests verify that when PromptOps determines a query is complex or
    research-level (requires_hrm=True), the orchestrator automatically enables
    hierarchical planning without requiring manual enable_hrm flag.
    """
    
    @pytest.fixture
    def complex_research_request(self):
        """Create a complex research query request."""
        return ChatRequest(
            prompt=(
                "Conduct a comprehensive analysis comparing the economic, environmental, "
                "and social impacts of renewable energy adoption across Germany, China, "
                "and the United States over the past decade. Include statistical data, "
                "policy comparisons, and future projections for each country."
            ),
            models=["gpt-4o", "claude-3-opus"],
            reasoning_mode=ReasoningMode.deep,
            domain_pack=DomainPack.research,
            agent_mode=AgentMode.team,
            orchestration=OrchestrationSettings(
                accuracy_level=4,
                enable_hrm=False,  # Explicitly NOT enabled - should auto-enable
            ),
        )
    
    @pytest.fixture
    def simple_request(self):
        """Create a simple query request."""
        return ChatRequest(
            prompt="What is the capital of France?",
            models=["gpt-4o"],
            reasoning_mode=ReasoningMode.standard,
            domain_pack=DomainPack.default,
            agent_mode=AgentMode.single,  # Use 'single' not 'solo'
            orchestration=OrchestrationSettings(
                accuracy_level=2,
                enable_hrm=False,
            ),
        )
    
    def test_complex_query_auto_enables_hrm(self, mock_providers, complex_research_request):
        """Test that complex queries automatically enable HRM."""
        # Mock PromptOps to return requires_hrm=True
        mock_prompt_spec = MagicMock()
        mock_prompt_spec.requires_hrm = True
        mock_prompt_spec.refined_query = complex_research_request.prompt
        mock_prompt_spec.confidence = 0.9
        mock_prompt_spec.safety_flags = []
        mock_prompt_spec.analysis = MagicMock()
        mock_prompt_spec.analysis.complexity = MagicMock(value="research")
        mock_prompt_spec.analysis.task_type = MagicMock(value="research")
        mock_prompt_spec.analysis.ambiguities = []
        mock_prompt_spec.analysis.tool_hints = []
        mock_prompt_spec.analysis.output_format = None
        
        # Simulate the config assembly and auto-enable logic
        orchestration_config = {
            "use_hrm": complex_research_request.orchestration.enable_hrm,
            "accuracy_level": complex_research_request.orchestration.accuracy_level,
        }
        
        # This is the actual auto-enable logic from orchestrator_adapter.py
        if mock_prompt_spec and hasattr(mock_prompt_spec, 'requires_hrm') and mock_prompt_spec.requires_hrm:
            if not orchestration_config.get("use_hrm", False):
                orchestration_config["use_hrm"] = True
        
        # Assert HRM was auto-enabled
        assert orchestration_config["use_hrm"] is True
    
    def test_simple_query_no_auto_hrm(self, mock_providers, simple_request):
        """Test that simple queries do NOT automatically enable HRM."""
        mock_prompt_spec = MagicMock()
        mock_prompt_spec.requires_hrm = False
        mock_prompt_spec.refined_query = simple_request.prompt
        mock_prompt_spec.analysis = MagicMock()
        mock_prompt_spec.analysis.complexity = MagicMock(value="simple")
        
        orchestration_config = {
            "use_hrm": simple_request.orchestration.enable_hrm,
            "accuracy_level": simple_request.orchestration.accuracy_level,
        }
        
        if mock_prompt_spec and hasattr(mock_prompt_spec, 'requires_hrm') and mock_prompt_spec.requires_hrm:
            if not orchestration_config.get("use_hrm", False):
                orchestration_config["use_hrm"] = True
        
        # Assert HRM was NOT enabled
        assert orchestration_config["use_hrm"] is False
    
    def test_hrm_bypasses_elite_orchestration(self, mock_providers, complex_research_request):
        """Test that HRM-enabled requests bypass elite orchestration."""
        # Simulate auto-enabled HRM
        orchestration_config = {
            "use_hrm": True,  # Auto-enabled for complex query
            "accuracy_level": 4,
        }
        
        # Verify elite orchestration would be bypassed
        ELITE_AVAILABLE = True
        is_team_mode = True
        accuracy_level = orchestration_config["accuracy_level"]
        actual_models = ["gpt-4o", "claude-3-opus"]
        use_hrm = orchestration_config["use_hrm"]
        
        use_elite = (
            ELITE_AVAILABLE and
            is_team_mode and
            accuracy_level >= 3 and
            len(actual_models) >= 2 and
            not use_hrm  # HRM takes precedence
        )
        
        assert use_elite is False
    
    def test_response_shows_hrm_enabled(self, mock_providers, complex_research_request):
        """Test that response extra shows hrm_enabled=True for complex queries."""
        # Simulate response construction with auto-enabled HRM
        orchestration_config = {
            "use_hrm": True,
            "accuracy_level": 4,
        }
        
        # The extra dict should show HRM was enabled
        extra = {
            "orchestration_settings": {
                "accuracy_level": orchestration_config.get("accuracy_level", 3),
                "hrm_enabled": orchestration_config.get("use_hrm", False),
            }
        }
        
        assert extra["orchestration_settings"]["hrm_enabled"] is True
    
    def test_verification_runs_after_hrm(self, mock_providers, complex_research_request):
        """Test that verification pipeline still runs after HRM execution."""
        # This test documents that HRM results go through verification
        # The HRM execution result's final_answer is wrapped in LLMResult
        # which then goes through the normal verification/refinement flow
        
        class MockHRMResult:
            success = True
            final_answer = "Comprehensive analysis result from hierarchical planning"
            final_model = "gpt-4o"
            total_tokens = 2000
            steps_completed = 5
            transparency_notes = ["Step 1: Decomposed query", "Step 2: Research data"]
            step_results = []
            total_latency_ms = 5000
            blackboard = None
        
        hrm_result = MockHRMResult()
        
        # Verify the HRM result can be processed
        assert hrm_result.success is True
        assert len(hrm_result.final_answer) > 0
        
        # The answer would then go through verification
        # (tested implicitly through the orchestrator's verification flow)
        should_verify = True  # For research tasks
        if should_verify:
            # Verification would run here in the actual code
            pass
    
    def test_multi_step_decomposition_indicators(self, mock_providers, complex_research_request):
        """Test that agent_traces show hierarchical planning was used."""
        # Simulate agent traces from HRM execution
        agent_traces = [
            {"agent": "planner", "action": "decompose", "step": 1, "description": "Analyze German renewable energy"},
            {"agent": "researcher", "action": "gather", "step": 2, "description": "Collect economic data"},
            {"agent": "researcher", "action": "gather", "step": 3, "description": "Collect environmental data"},
            {"agent": "analyst", "action": "compare", "step": 4, "description": "Cross-country comparison"},
            {"agent": "synthesizer", "action": "compose", "step": 5, "description": "Final synthesis"},
        ]
        
        # Verify multiple steps were executed
        assert len(agent_traces) >= 3  # HRM should have multiple steps
        
        # Verify planning/decomposition occurred
        planner_traces = [t for t in agent_traces if t.get("agent") == "planner"]
        assert len(planner_traces) >= 1


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
