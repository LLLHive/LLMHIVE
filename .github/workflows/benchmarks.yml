name: Performance Benchmarks

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      compare_baseline:
        description: 'Compare against baseline'
        required: false
        default: 'true'

env:
  PYTHON_VERSION: '3.12'

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for comparison
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          cd llmhive
          pip install -r requirements.txt
          pip install pytest-benchmark psutil
      
      - name: Download baseline (if exists)
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: benchmark-baseline
          path: baseline/
      
      - name: Run benchmarks
        run: |
          cd llmhive
          PYTHONPATH=./src pytest tests/benchmarks/test_orchestration_performance.py \
            -v \
            --benchmark-json=benchmark-results.json \
            --benchmark-save=current \
            --benchmark-warmup=on \
            --benchmark-min-rounds=3 \
            --tb=short \
            || true  # Don't fail on benchmark-only failures
      
      - name: Compare with baseline
        if: hashFiles('baseline/benchmark-baseline.json') != ''
        run: |
          cd llmhive
          python -c "
          import json
          import sys
          
          # Load baseline and current results
          try:
              with open('../baseline/benchmark-baseline.json') as f:
                  baseline = json.load(f)
              with open('benchmark-results.json') as f:
                  current = json.load(f)
          except FileNotFoundError as e:
              print(f'File not found: {e}')
              sys.exit(0)
          
          # Compare benchmarks
          regressions = []
          improvements = []
          
          baseline_benchmarks = {b['name']: b for b in baseline.get('benchmarks', [])}
          current_benchmarks = {b['name']: b for b in current.get('benchmarks', [])}
          
          for name, curr in current_benchmarks.items():
              if name in baseline_benchmarks:
                  base = baseline_benchmarks[name]
                  
                  curr_mean = curr['stats']['mean']
                  base_mean = base['stats']['mean']
                  
                  change_pct = ((curr_mean - base_mean) / base_mean) * 100
                  
                  if change_pct > 10:  # >10% regression
                      regressions.append({
                          'name': name,
                          'baseline': base_mean,
                          'current': curr_mean,
                          'change': change_pct
                      })
                  elif change_pct < -10:  # >10% improvement
                      improvements.append({
                          'name': name,
                          'baseline': base_mean,
                          'current': curr_mean,
                          'change': change_pct
                      })
          
          # Print results
          print('=== Benchmark Comparison ===')
          
          if improvements:
              print('\nâœ… Improvements (>10% faster):')
              for imp in improvements:
                  print(f\"  {imp['name']}: {imp['change']:.1f}% ({imp['baseline']*1000:.2f}ms â†’ {imp['current']*1000:.2f}ms)\")
          
          if regressions:
              print('\nâŒ Regressions (>10% slower):')
              for reg in regressions:
                  print(f\"  {reg['name']}: +{reg['change']:.1f}% ({reg['baseline']*1000:.2f}ms â†’ {reg['current']*1000:.2f}ms)\")
              print('\nâš ï¸  Performance regression detected!')
              sys.exit(1)
          
          if not improvements and not regressions:
              print('\nâœ… No significant changes detected')
          "
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: llmhive/benchmark-results.json
          retention-days: 30
      
      - name: Update baseline (main branch only)
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-baseline
          path: llmhive/benchmark-results.json
          retention-days: 90
      
      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let results;
            try {
              results = JSON.parse(fs.readFileSync('llmhive/benchmark-results.json', 'utf8'));
            } catch (e) {
              console.log('No benchmark results found');
              return;
            }
            
            const benchmarks = results.benchmarks || [];
            
            let body = '## ðŸ“Š Performance Benchmark Results\n\n';
            body += '| Benchmark | Mean | Min | Max | Std Dev |\n';
            body += '|-----------|------|-----|-----|--------|\n';
            
            benchmarks.forEach(b => {
              const stats = b.stats;
              body += `| ${b.name} | ${(stats.mean * 1000).toFixed(2)}ms | ${(stats.min * 1000).toFixed(2)}ms | ${(stats.max * 1000).toFixed(2)}ms | ${(stats.stddev * 1000).toFixed(2)}ms |\n`;
            });
            
            body += '\n_Run on commit: ' + context.sha.substring(0, 7) + '_';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });

  performance-report:
    name: Generate Performance Report
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results
          path: results/
      
      - name: Generate report
        run: |
          echo "## Performance Report" > report.md
          echo "" >> report.md
          echo "Generated at: $(date -u)" >> report.md
          echo "" >> report.md
          
          if [ -f results/benchmark-results.json ]; then
            python3 -c "
          import json
          
          with open('results/benchmark-results.json') as f:
              data = json.load(f)
          
          benchmarks = data.get('benchmarks', [])
          
          print('### Benchmark Results\n')
          print('| Benchmark | Mean (ms) | Rounds |')
          print('|-----------|-----------|--------|')
          
          for b in benchmarks:
              name = b['name']
              mean_ms = b['stats']['mean'] * 1000
              rounds = b['stats']['rounds']
              print(f'| {name} | {mean_ms:.2f} | {rounds} |')
          " >> report.md
          fi
          
          cat report.md
      
      - name: Upload report
        uses: actions/upload-artifact@v4
        with:
          name: performance-report
          path: report.md
          retention-days: 30
