name: Benchmark Comparison (Multi-System)

on:
  workflow_dispatch:
    inputs:
      systems:
        description: 'Systems to benchmark (comma-separated)'
        required: false
        default: 'llmhive'
      runs_per_case:
        description: 'Number of runs per case'
        required: false
        default: '1'
      enable_external:
        description: 'Enable external API calls (requires secrets)'
        required: false
        default: 'false'
        type: boolean
  
  schedule:
    # Run weekly on Sunday at 2am UTC
    - cron: '0 2 * * 0'
  
  push:
    branches: [main]
    paths:
      - 'benchmarks/**'
      - 'llmhive/src/llmhive/app/benchmarks/**'
      - 'tests/benchmarks/**'

env:
  PYTHON_VERSION: '3.12'

jobs:
  # ============================================================================
  # Job 1: Unit Tests (always run, no external calls)
  # ============================================================================
  unit-tests:
    name: Benchmark Unit Tests
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          cd llmhive
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pyyaml
      
      - name: Run quality eval tests
        run: |
          cd llmhive
          PYTHONPATH=./src pytest tests/quality_eval -q --tb=short || true
      
      - name: Run benchmark unit tests
        run: |
          cd llmhive
          PYTHONPATH=./src pytest tests/benchmarks -q --tb=short

  # ============================================================================
  # Job 2: LLMHive Local Benchmark (no external API keys needed)
  # ============================================================================
  benchmark-local:
    name: LLMHive Local Benchmark
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          cd llmhive
          pip install -r requirements.txt
          pip install pyyaml httpx
      
      - name: Run LLMHive benchmark
        id: benchmark
        run: |
          cd llmhive
          mkdir -p ../artifacts/benchmarks
          
          # Run benchmark with LLMHive only (no external APIs)
          PYTHONPATH=./src python -m llmhive.app.benchmarks.cli \
            --suite ../benchmarks/suites/complex_reasoning_v1.yaml \
            --systems llmhive \
            --mode local \
            --runs-per-case 1 \
            --outdir ../artifacts/benchmarks/local_$(date +%Y%m%d_%H%M%S) \
            --temperature 0.0 \
            --timeout 120 \
            2>&1 | tee benchmark-output.log
          
          # Check for critical failures
          if grep -q "❌ FAILED" benchmark-output.log; then
            echo "benchmark_passed=false" >> $GITHUB_OUTPUT
          else
            echo "benchmark_passed=true" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true
      
      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-local-results
          path: artifacts/benchmarks/
          retention-days: 30
      
      - name: Quality Gate - Check Critical Failures
        if: steps.benchmark.outputs.benchmark_passed == 'false'
        run: |
          echo "::error::LLMHive benchmark has critical failures!"
          exit 1

  # ============================================================================
  # Job 3: Multi-System Benchmark (requires secrets, manual dispatch only)
  # ============================================================================
  benchmark-external:
    name: Multi-System Benchmark
    runs-on: ubuntu-latest
    needs: unit-tests
    if: |
      (github.event_name == 'workflow_dispatch' && github.event.inputs.enable_external == 'true') ||
      (github.event_name == 'schedule' && github.repository_owner == 'your-org')
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          cd llmhive
          pip install -r requirements.txt
          pip install pyyaml httpx openai anthropic
      
      - name: Check available secrets
        id: secrets
        run: |
          systems="llmhive"
          
          if [ -n "${{ secrets.OPENAI_API_KEY }}" ]; then
            systems="${systems},openai"
            echo "openai_available=true" >> $GITHUB_OUTPUT
          else
            echo "::notice::OPENAI_API_KEY not configured - skipping OpenAI baseline"
            echo "openai_available=false" >> $GITHUB_OUTPUT
          fi
          
          if [ -n "${{ secrets.ANTHROPIC_API_KEY }}" ]; then
            systems="${systems},anthropic"
            echo "anthropic_available=true" >> $GITHUB_OUTPUT
          else
            echo "::notice::ANTHROPIC_API_KEY not configured - skipping Anthropic baseline"
            echo "anthropic_available=false" >> $GITHUB_OUTPUT
          fi
          
          if [ -n "${{ secrets.PERPLEXITY_API_KEY }}" ]; then
            systems="${systems},perplexity"
            echo "perplexity_available=true" >> $GITHUB_OUTPUT
          else
            echo "::notice::PERPLEXITY_API_KEY not configured - skipping Perplexity baseline"
            echo "perplexity_available=false" >> $GITHUB_OUTPUT
          fi
          
          echo "systems=${systems}" >> $GITHUB_OUTPUT
          echo "Available systems: ${systems}"
      
      - name: Run multi-system benchmark
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          PERPLEXITY_API_KEY: ${{ secrets.PERPLEXITY_API_KEY }}
        run: |
          cd llmhive
          mkdir -p ../artifacts/benchmarks
          
          # Use systems determined by available secrets
          SYSTEMS="${{ steps.secrets.outputs.systems }}"
          
          # Override with user input if provided
          if [ "${{ github.event.inputs.systems }}" != "" ] && [ "${{ github.event.inputs.systems }}" != "llmhive" ]; then
            SYSTEMS="${{ github.event.inputs.systems }}"
          fi
          
          RUNS="${{ github.event.inputs.runs_per_case || '1' }}"
          
          echo "Running benchmark with systems: ${SYSTEMS}, runs: ${RUNS}"
          
          PYTHONPATH=./src python -m llmhive.app.benchmarks.cli \
            --suite ../benchmarks/suites/complex_reasoning_v1.yaml \
            --systems "${SYSTEMS}" \
            --mode local \
            --runs-per-case "${RUNS}" \
            --outdir ../artifacts/benchmarks/multi_$(date +%Y%m%d_%H%M%S) \
            --temperature 0.0 \
            --timeout 180 \
            2>&1 | tee benchmark-output.log
        continue-on-error: true
      
      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-multi-results
          path: artifacts/benchmarks/
          retention-days: 90
      
      - name: Generate comparison summary
        run: |
          # Find the latest report
          REPORT=$(find artifacts/benchmarks -name "report.md" -type f | head -1)
          
          if [ -f "$REPORT" ]; then
            echo "## Benchmark Comparison Results" > summary.md
            echo "" >> summary.md
            cat "$REPORT" >> summary.md
            cat summary.md
          else
            echo "No report generated"
          fi

  # ============================================================================
  # Job 4: Golden Regression Gate (runs on main push)
  # ============================================================================
  golden-regression:
    name: Golden Prompt Regression
    runs-on: ubuntu-latest
    needs: unit-tests
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          cd llmhive
          pip install -r requirements.txt
          pip install pyyaml
      
      - name: Run golden prompt audit
        env:
          GOLDEN_PROMPT_AUDIT: 'true'
        run: |
          cd llmhive
          
          # Run subset of golden prompts for regression check
          PYTHONPATH=./src python -c "
          import asyncio
          import sys
          sys.path.insert(0, 'src')
          
          # Import audit script
          try:
              from tests.quality_eval.live_prompt_audit import run_audit
              
              # Run audit with limited prompts
              result = asyncio.run(run_audit(
                  mode='local',
                  max_prompts=20,
                  require_real_provider=False,
              ))
              
              # Check failure rate
              if result.get('failure_rate', 0) > 0.3:
                  print(f'::error::Golden prompt failure rate too high: {result[\"failure_rate\"]:.1%}')
                  sys.exit(1)
              else:
                  print(f'✅ Golden prompt audit passed (failure rate: {result.get(\"failure_rate\", 0):.1%})')
          except ImportError as e:
              print(f'::warning::Golden prompt audit not available: {e}')
          except Exception as e:
              print(f'::warning::Golden prompt audit failed: {e}')
          "
        continue-on-error: true

  # ============================================================================
  # Summary
  # ============================================================================
  summary:
    name: Benchmark Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, benchmark-local]
    if: always()
    
    steps:
      - name: Check results
        run: |
          echo "## Benchmark Summary"
          echo ""
          echo "Unit Tests: ${{ needs.unit-tests.result }}"
          echo "Local Benchmark: ${{ needs.benchmark-local.result }}"
          
          if [ "${{ needs.unit-tests.result }}" != "success" ]; then
            echo "::error::Unit tests failed!"
            exit 1
          fi
          
          if [ "${{ needs.benchmark-local.result }}" == "failure" ]; then
            echo "::error::Local benchmark has critical failures!"
            exit 1
          fi
          
          echo "✅ All benchmark checks passed"

