# LLMHive Industry Benchmark Report
**Timestamp:** 2026-02-08T00:16:09.117577
**Tier:** elite
**Reasoning Mode:** deep
**Strict Mode:** OFF

## Category Summary
| Category | Avg Accuracy | Std Dev | Avg Cost/Attempt | Dataset |
|----------|--------------|---------|------------------|---------|
| General Reasoning | 70.81% | 0.00% | $0.002575 | MMLU |
| Math | 91.96% | 0.00% | $0.007501 | GSM8K |
| Coding | 0.00% | 0.00% | $0.003351 | HumanEval |
| RAG | 0.00% | 0.00% | $0.433140 | MS MARCO |
| Tool Use | 0.00% | 0.00% | $0.000000 | ToolBench |

## Detailed Runs
### General Reasoning
| Run | Accuracy | Attempted | Errors | Avg Latency | Total Cost |
|-----|----------|-----------|--------|-------------|------------|
| 1 | 70.81% | 14042 | 0 | 7201ms | $36.1547 |

### Math
| Run | Accuracy | Attempted | Errors | Avg Latency | Total Cost |
|-----|----------|-----------|--------|-------------|------------|
| 1 | 91.96% | 1319 | 0 | 8584ms | $9.8944 |

### Coding
| Run | Accuracy | Attempted | Errors | Avg Latency | Total Cost |
|-----|----------|-----------|--------|-------------|------------|
| 1 | 0.00% | 164 | 0 | 4323ms | $0.5496 |

### RAG
| Run | Accuracy | Attempted | Errors | Avg Latency | Total Cost |
|-----|----------|-----------|--------|-------------|------------|
| 1 | 0.00% | 10 | 0 | 375361ms | $4.3314 |
- Avg F1 across runs: 0.00%
- Avg MRR@10 across runs: 0.0000

### Tool Use
| Run | Accuracy | Attempted | Errors | Avg Latency | Total Cost |
|-----|----------|-----------|--------|-------------|------------|
| 1 | 0.00% | 0 | 1 | 0ms | $0.0000 |
- ToolBench note: ToolEval failed: [Errno 2] No such file or directory: '...'
