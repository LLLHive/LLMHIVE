{
  "tier": "elite",
  "results": [
    {
      "category": "General Reasoning (MMLU)",
      "dataset": "lighteval/mmlu",
      "sample_size": 100,
      "correct": 75,
      "incorrect": 25,
      "errors": 0,
      "accuracy": 75.0,
      "accuracy_before_rerun": 75.0,
      "infra_failures_before_rerun": 0,
      "infra_failures_after_rerun": 0,
      "rerun_recovered": 0,
      "avg_latency_ms": 5200,
      "avg_cost": 0.00208,
      "total_cost": 0.208,
      "extra": {
        "error_samples": [],
        "subject_stats": {
          "business_ethics": {
            "correct": 3,
            "total": 3
          },
          "miscellaneous": {
            "correct": 12,
            "total": 13
          },
          "professional_law": {
            "correct": 3,
            "total": 10
          },
          "professional_psychology": {
            "correct": 5,
            "total": 7
          },
          "moral_scenarios": {
            "correct": 4,
            "total": 5
          },
          "high_school_biology": {
            "correct": 2,
            "total": 3
          },
          "high_school_statistics": {
            "correct": 1,
            "total": 2
          },
          "high_school_psychology": {
            "correct": 4,
            "total": 4
          },
          "college_medicine": {
            "correct": 4,
            "total": 4
          },
          "elementary_mathematics": {
            "correct": 3,
            "total": 3
          },
          "human_sexuality": {
            "correct": 1,
            "total": 1
          },
          "philosophy": {
            "correct": 2,
            "total": 3
          },
          "high_school_world_history": {
            "correct": 1,
            "total": 2
          },
          "anatomy": {
            "correct": 1,
            "total": 1
          },
          "sociology": {
            "correct": 4,
            "total": 5
          },
          "nutrition": {
            "correct": 1,
            "total": 3
          },
          "logical_fallacies": {
            "correct": 2,
            "total": 2
          },
          "high_school_geography": {
            "correct": 0,
            "total": 2
          },
          "high_school_european_history": {
            "correct": 1,
            "total": 1
          },
          "professional_medicine": {
            "correct": 1,
            "total": 2
          },
          "high_school_macroeconomics": {
            "correct": 2,
            "total": 2
          },
          "moral_disputes": {
            "correct": 1,
            "total": 3
          },
          "world_religions": {
            "correct": 2,
            "total": 2
          },
          "professional_accounting": {
            "correct": 1,
            "total": 1
          },
          "high_school_us_history": {
            "correct": 0,
            "total": 2
          },
          "jurisprudence": {
            "correct": 1,
            "total": 1
          },
          "international_law": {
            "correct": 1,
            "total": 1
          },
          "conceptual_physics": {
            "correct": 2,
            "total": 2
          },
          "formal_logic": {
            "correct": 1,
            "total": 2
          },
          "marketing": {
            "correct": 1,
            "total": 1
          },
          "clinical_knowledge": {
            "correct": 0,
            "total": 1
          },
          "global_facts": {
            "correct": 1,
            "total": 1
          },
          "human_aging": {
            "correct": 0,
            "total": 1
          },
          "high_school_government_and_politics": {
            "correct": 1,
            "total": 1
          },
          "high_school_computer_science": {
            "correct": 1,
            "total": 1
          },
          "virology": {
            "correct": 0,
            "total": 1
          },
          "college_computer_science": {
            "correct": 1,
            "total": 1
          }
        }
      },
      "exec_integrity": {
        "attempted": 408,
        "errors": 44,
        "infra_failures": 0,
        "retries": 20,
        "fallback_used": 20
      },
      "infra_failures": 0
    },
    {
      "category": "Math (GSM8K)",
      "dataset": "openai/gsm8k",
      "sample_size": 100,
      "correct": 94,
      "incorrect": 6,
      "errors": 0,
      "accuracy": 94.0,
      "accuracy_before_rerun": 93.9,
      "infra_failures_before_rerun": 1,
      "infra_failures_after_rerun": 0,
      "rerun_recovered": 1,
      "avg_latency_ms": 24878,
      "avg_cost": 0.0,
      "total_cost": 0.0,
      "extra": {
        "error_samples": [
          "No valid answer from generate-then-verify"
        ]
      },
      "exec_integrity": {
        "attempted": 1674,
        "errors": 46,
        "infra_failures": 0,
        "retries": 41,
        "fallback_used": 41
      },
      "infra_failures": 0
    }
  ],
  "timestamp": "2026-02-28T09:35:08.619905",
  "invariants_verified": true
}