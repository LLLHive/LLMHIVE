# LLMHive ELITE Tier: 8-Category Industry Benchmark
**Test Date:** February 08, 2026
**API:** https://llmhive-orchestrator-792354158895.us-east1.run.app
**Reasoning Mode:** deep
**Strict Mode:** OFF

---

## üéØ Executive Summary

**Overall Accuracy:** 37.6% (168/447)
**Total Cost:** $3.0573
**Average Cost per Category:** $0.3822
**Categories Tested:** 8

## üìä Category Results

| Category | Score | Dataset | Status |
|----------|-------|---------|--------|
| General Reasoning (MMLU) | **70.0%** | lighteval/mmlu | ‚ö†Ô∏è |
| Coding (HumanEval) | **6.0%** | openai/human_eval | ‚ùå |
| Math (GSM8K) | **94.0%** | openai/gsm8k | ‚úÖ |
| Multilingual (MMMLU) | **0.0%** | openai/MMMLU | ‚ùå |
| Long Context (LongBench) | **0.0%** | THUDM/LongBench | ‚ùå |
| Tool Use (ToolBench) | **0.0%** | ToolBench - SKIPPED | ‚ùå |
| RAG (MS MARCO) | **0.5%** | microsoft/ms_marco v1.1 | ‚ùå |
| Dialogue (MT-Bench) | **0.0%** | lmsys/mt-bench - SKIPPED | ‚ùå |

---

## üìã Detailed Results

### General Reasoning (MMLU)

- **Dataset:** lighteval/mmlu
- **Sample Size:** 100
- **Correct:** 70/100 (70.0%)
- **Errors:** 0
- **Avg Latency:** 2143ms
- **Avg Cost:** $0.002306
- **Total Cost:** $0.2306

### Coding (HumanEval)

- **Dataset:** openai/human_eval
- **Sample Size:** 50
- **Correct:** 3/50 (6.0%)
- **Errors:** 0
- **Avg Latency:** 2772ms
- **Avg Cost:** $0.003084
- **Total Cost:** $0.1542

### Math (GSM8K)

- **Dataset:** openai/gsm8k
- **Sample Size:** 100
- **Correct:** 94/100 (94.0%)
- **Errors:** 0
- **Avg Latency:** 4538ms
- **Avg Cost:** $0.007149
- **Total Cost:** $0.7149

### Multilingual (MMMLU)

- **Dataset:** openai/MMMLU
- **Sample Size:** 100
- **Correct:** 0/0 (0.0%)
- **Errors:** 100
- **Avg Latency:** 0ms
- **Avg Cost:** $0.000000
- **Total Cost:** $0.0000

### Long Context (LongBench)

- **Dataset:** THUDM/LongBench
- **Sample Size:** 0
- **Correct:** 0/-1 (0.0%)
- **Errors:** 1
- **Avg Latency:** 0ms
- **Avg Cost:** $0.000000
- **Total Cost:** $0.0000

### Tool Use (ToolBench)

- **Dataset:** ToolBench - SKIPPED
- **Sample Size:** 0
- **Correct:** 0/-1 (0.0%)
- **Errors:** 1
- **Avg Latency:** 0ms
- **Avg Cost:** $0.000000
- **Total Cost:** $0.0000

### RAG (MS MARCO)

- **Dataset:** microsoft/ms_marco v1.1
- **Sample Size:** 200
- **Correct:** 1/200 (0.5%)
- **Errors:** 0
- **Avg Latency:** 3498ms
- **Avg Cost:** $0.009788
- **Total Cost:** $1.9576

### Dialogue (MT-Bench)

- **Dataset:** lmsys/mt-bench - SKIPPED
- **Sample Size:** 0
- **Correct:** 0/-1 (0.0%)
- **Errors:** 1
- **Avg Latency:** 0ms
- **Avg Cost:** $0.000000
- **Total Cost:** $0.0000


---

**Report Generated:** 2026-02-08T16:23:19.233760
**Status:** ELITE Tier Benchmarked