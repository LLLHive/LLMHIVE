# LLMHive ELITE Tier: 8-Category Industry Benchmark
**Test Date:** February 23, 2026
**API:** https://llmhive-orchestrator-792354158895.us-east1.run.app
**Reasoning Mode:** deep
**Strict Mode:** OFF

---

## üéØ Executive Summary

**Overall Accuracy:** 93.2% (507/544)
**Total Cost:** $7.8156
**Average Cost per Category:** $0.9769
**Categories Tested:** 8

## üìä Category Results

| Category | Score | Dataset | Status |
|----------|-------|---------|--------|
| General Reasoning (MMLU) | **74.0%** | lighteval/mmlu | ‚ö†Ô∏è |
| Coding (HumanEval) | **88.0%** | openai/human_eval | ‚úÖ |
| Math (GSM8K) | **93.7%** | openai/gsm8k | ‚úÖ |
| Multilingual (MMMLU) | **81.0%** | openai/MMMLU | ‚úÖ |
| Long Context (LongBench) | **100.0%** | THUDM/LongBench | ‚úÖ |
| Tool Use (ToolBench) | **100.0%** | ToolBench (OpenBMB) | ‚úÖ |
| RAG (MS MARCO) | **46.3%** | microsoft/ms_marco v1.1 | ‚ùå |
| Dialogue (MT-Bench) | **4.4%** | lmsys/mt-bench | ‚ùå |

---

## üìã Detailed Results

### General Reasoning (MMLU)

- **Dataset:** lighteval/mmlu
- **Sample Size:** 100
- **Correct:** 74/100 (74.0%)
- **Errors:** 0
- **Avg Latency:** 5000ms
- **Avg Cost:** $0.005000
- **Total Cost:** $0.5000

### Coding (HumanEval)

- **Dataset:** openai/human_eval
- **Sample Size:** 50
- **Correct:** 44/50 (88.0%)
- **Errors:** 0
- **Avg Latency:** 42890ms
- **Avg Cost:** $0.041777
- **Total Cost:** $2.0889

### Math (GSM8K)

- **Dataset:** openai/gsm8k
- **Sample Size:** 100
- **Correct:** 89/95 (93.7%)
- **Errors:** 5
- **Avg Latency:** 23546ms
- **Avg Cost:** $0.018486
- **Total Cost:** $1.7561

### Multilingual (MMMLU)

- **Dataset:** openai/MMMLU
- **Sample Size:** 100
- **Correct:** 81/100 (81.0%)
- **Errors:** 0
- **Avg Latency:** 17432ms
- **Avg Cost:** $0.005735
- **Total Cost:** $0.5735

### Long Context (LongBench)

- **Dataset:** THUDM/LongBench
- **Sample Size:** 20
- **Correct:** 20/20 (100.0%)
- **Errors:** 0
- **Avg Latency:** 12787ms
- **Avg Cost:** $0.000000
- **Total Cost:** $0.0000

### Tool Use (ToolBench)

- **Dataset:** ToolBench (OpenBMB)
- **Sample Size:** 10
- **Correct:** 10/10 (100.0%)
- **Errors:** 0
- **Avg Latency:** 6789ms
- **Avg Cost:** $0.000000
- **Total Cost:** $0.0000

### RAG (MS MARCO)

- **Dataset:** microsoft/ms_marco v1.1
- **Sample Size:** 200
- **Correct:** 189/159 (46.3%)
- **Errors:** 41
- **Avg Latency:** 18139ms
- **Avg Cost:** $0.018221
- **Total Cost:** $2.8971

### Dialogue (MT-Bench)

- **Dataset:** lmsys/mt-bench
- **Sample Size:** 10
- **Correct:** 0/10 (4.4%)
- **Errors:** 0
- **Avg Latency:** 67441ms
- **Avg Cost:** $0.000000
- **Total Cost:** $0.0000

## üí∞ Cost Analysis

| Category | Total Cost | Cost/Correct | Cost/Sample | Samples |
|----------|-----------|-------------|------------|---------|
| General Reasoning (MMLU) | $0.5000 | $0.0068 | $0.0050 | 100 |
| Coding (HumanEval) | $2.0889 | $0.0475 | $0.0418 | 50 |
| Math (GSM8K) | $1.7561 | $0.0197 | $0.0176 | 100 |
| Multilingual (MMMLU) | $0.5735 | $0.0071 | $0.0057 | 100 |
| Long Context (LongBench) | $0.0000 | $0.0000 | $0.0000 | 20 |
| Tool Use (ToolBench) | $0.0000 | $0.0000 | $0.0000 | 10 |
| RAG (MS MARCO) | $2.8971 | $0.0153 | $0.0145 | 200 |
| Dialogue (MT-Bench) | $0.0000 | $0.0000 | $0.0000 | 10 |
| **TOTAL** | **$7.8156** | **$0.0154** | **$0.0132** | **590** |


---

**Report Generated:** 2026-02-23T09:29:50.171509
**Status:** ELITE Tier Benchmarked