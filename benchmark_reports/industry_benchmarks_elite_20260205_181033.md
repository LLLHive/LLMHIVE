# LLMHive Industry Benchmark Report
**Timestamp:** 2026-02-05T18:10:33.692467
**Tier:** elite
**Reasoning Mode:** deep

## Category Summary
| Category | Avg Accuracy | Std Dev | Avg Cost/Attempt | Dataset |
|----------|--------------|---------|------------------|---------|
| General Reasoning | 43.00% | 0.00% | $0.001606 | MMLU |
| Math | 89.00% | 0.00% | $0.007436 | GSM8K |
| Coding | 0.00% | 0.00% | $0.003470 | HumanEval |
| RAG | 0.00% | 0.00% | $0.003466 | MS MARCO |
| Tool Use | 0.00% | 0.00% | $0.000000 | ToolBench |

## Detailed Runs
### General Reasoning
| Run | Accuracy | Attempted | Errors | Avg Latency | Total Cost |
|-----|----------|-----------|--------|-------------|------------|
| 1 | 43.00% | 100 | 0 | 1930ms | $0.1606 |

### Math
| Run | Accuracy | Attempted | Errors | Avg Latency | Total Cost |
|-----|----------|-----------|--------|-------------|------------|
| 1 | 89.00% | 100 | 0 | 10085ms | $0.7436 |

### Coding
| Run | Accuracy | Attempted | Errors | Avg Latency | Total Cost |
|-----|----------|-----------|--------|-------------|------------|
| 1 | 0.00% | 50 | 0 | 4411ms | $0.1735 |

### RAG
| Run | Accuracy | Attempted | Errors | Avg Latency | Total Cost |
|-----|----------|-----------|--------|-------------|------------|
| 1 | 0.00% | 100 | 0 | 4075ms | $0.3466 |
- Avg F1 across runs: 27.23%

### Tool Use
| Run | Accuracy | Attempted | Errors | Avg Latency | Total Cost |
|-----|----------|-----------|--------|-------------|------------|
| 1 | 0.00% | 0 | 1 | 0ms | $0.0000 |
- ToolBench note: ToolEval not implemented in this script
