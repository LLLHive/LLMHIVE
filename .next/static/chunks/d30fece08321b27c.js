(globalThis.TURBOPACK||(globalThis.TURBOPACK=[])).push(["object"==typeof document?document.currentScript:void 0,66291,e=>{"use strict";var r=e.i(43476),s=e.i(71645),t=e.i(18566),a=e.i(57688),n=e.i(10708),o=e.i(90382),i=e.i(94179),l=e.i(71435),c=e.i(52917),d=e.i(66992),m=e.i(55711),u=e.i(55248),p=e.i(95116),h=e.i(75254);let g=(0,h.default)("Hammer",[["path",{d:"m15 12-8.373 8.373a1 1 0 1 1-3-3L12 9",key:"eefl8a"}],["path",{d:"m18 15 4-4",key:"16gjal"}],["path",{d:"m21.5 11.5-1.914-1.914A2 2 0 0 1 19 8.172V7l-2.26-2.26a6 6 0 0 0-4.202-1.756L9 2.96l.92.82A6.18 6.18 0 0 1 12 8.4V10l2 2h1.172a2 2 0 0 1 1.414.586L18.5 14.5",key:"b7pghm"}]]);var x=e.i(52008),b=e.i(43531),f=e.i(78917),v=e.i(83086),y=e.i(39312);let j=(0,h.default)("Crown",[["path",{d:"M11.562 3.266a.5.5 0 0 1 .876 0L15.39 8.87a1 1 0 0 0 1.516.294L21.183 5.5a.5.5 0 0 1 .798.519l-2.834 10.246a1 1 0 0 1-.956.734H5.81a1 1 0 0 1-.957-.734L2.02 6.02a.5.5 0 0 1 .798-.519l4.276 3.664a1 1 0 0 0 1.516-.294z",key:"1vdc57"}],["path",{d:"M5 21h14",key:"11awu3"}]]);var w=e.i(98919),N=e.i(52453);let k=["Automatic","General Reasoning","Code Generation & Reasoning","Mathematical Reasoning","Commonsense & Logical Reasoning","Multi-Modal Reasoning"],S=[{id:"automatic",name:"Automatic",year:2025,category:"Automatic",shortDescription:"Let the LLMHive orchestrator automatically select the optimal reasoning method based on your query.",strengths:["Optimal method selection for each task","No configuration required","Adaptive to query complexity","Best overall performance"],weaknesses:["Less control over specific method used"],referenceUrl:"https://llmhive.ai/docs/automatic-reasoning",benchmarkResults:"Achieves best average performance across all benchmarks by dynamically selecting optimal methods"},{id:"cot",name:"Chain-of-Thought (CoT) Prompting",year:2022,category:"General Reasoning",shortDescription:"Breaks problems into steps via prompt engineering. Enables LMs to generate intermediate reasoning steps before the final answer.",strengths:["Improves accuracy on multi-step reasoning tasks","State-of-the-art on GSM8K math benchmark","Works across domains (math, logic, commonsense)"],weaknesses:["Increases token usage and latency","Mainly effective in sufficiently large models","May produce verbose reasoning"],referenceUrl:"https://arxiv.org/abs/2201.11903",benchmarkResults:"540B model with CoT achieved SOTA on GSM8K, surpassing fine-tuned GPT-3"},{id:"self-consistency",name:"Self-Consistency Decoding",year:2022,category:"General Reasoning",shortDescription:"Majority-vote over multiple reasoning paths. Samples diverse CoT trails and chooses the most common answer.",strengths:["+17.9% accuracy on GSM8K","+11% on SVAMP, +12% on AQuA","+6.4% on StrategyQA, +3.9% on ARC-Challenge"],weaknesses:["Higher compute cost (sampling many outputs)","Requires multiple inference passes","May not help if all paths share the same bias"],referenceUrl:"https://arxiv.org/abs/2203.11171",benchmarkResults:"Improved CoT performance on GSM8K by +17.9% absolute"},{id:"tree-of-thoughts",name:"Tree-of-Thoughts (ToT) Search",year:2023,category:"General Reasoning",shortDescription:"Deliberative branching of reasoning steps. Explores a tree of possible thought sequences and backtracks as needed.",strengths:["74% solve rate on Game of 24 (vs 4% with CoT)","Excels at planning and multi-hop reasoning","Systematically finds correct solutions"],weaknesses:["Extra prompting steps and state management","High computational overhead","Complex to implement and tune"],referenceUrl:"https://arxiv.org/abs/2305.10601",benchmarkResults:"GPT-4 with ToT solved 74% of Game of 24 challenges vs ~4% with standard CoT"},{id:"react",name:"ReAct (Reason+Act Framework)",year:2022,category:"General Reasoning",shortDescription:"Interleaving reasoning with tool use/actions. Produces reasoning traces alongside actions in an interwoven loop.",strengths:["35.1% exact-match on HotpotQA (vs 29% CoT)","10-34% improvement on interactive tasks","Human-like, interpretable chains"],weaknesses:["Requires external APIs or simulators","More complex prompting","Dependent on tool reliability"],referenceUrl:"https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/",benchmarkResults:"ReAct + CoT achieved 35.1% on HotpotQA vs ~29% with CoT alone"},{id:"pal",name:"Program-Aided Reasoning (PAL)",year:2022,category:"General Reasoning",shortDescription:"LLM generates code to solve problems. Outputs a Python program as the reasoning chain, executes it, returns the result.",strengths:["58.4% accuracy on GSM8K (vs 48.2% with CoT)","Eliminates final-step calculation errors","Verifiable reasoning via executed programs"],weaknesses:["Requires execution environment","Overhead of running code","Limited to problems expressible as code"],referenceUrl:"https://arxiv.org/abs/2211.10435",benchmarkResults:"PAL achieved 58.4% on GSM8K, surpassing 48.2% with plain CoT"},{id:"self-reflection",name:"Self-Reflection & Iterative Refinement",year:2023,category:"General Reasoning",shortDescription:"Models critique and refine their own outputs. Generates initial answer, inspects for errors, then improves upon it.",strengths:["88% pass@1 on HumanEval (vs 67% base GPT-4)","+12% accuracy on MBPP coding tasks","+9% on hardest Spider SQL queries"],weaknesses:["Increased computation from multiple generations","May over-correct or introduce new errors","Diminishing returns after few iterations"],referenceUrl:"https://arxiv.org/abs/2303.11366",benchmarkResults:"Reflexion-augmented GPT-4 achieved 88% pass@1 on HumanEval"},{id:"alphacode",name:"Massive Sample & Filter (AlphaCode)",year:2022,category:"Code Generation & Reasoning",shortDescription:"Generate many solutions and test them. Generates tens of thousands of solutions and filters by running against unit tests.",strengths:["Top 54.3% rank among human competitors","First AI to achieve median human performance","Robust against edge cases via filtering"],weaknesses:["Heavy computation (many samples)","Requires reliable test cases","Not practical for real-time applications"],referenceUrl:"https://deepmind.google/discover/blog/competitive-programming-with-alphacode/",benchmarkResults:"Achieved average rank in top 54.3% of Codeforces competitors"},{id:"self-debugging",name:"Self-Debugging & Automated Repair",year:2023,category:"Code Generation & Reasoning",shortDescription:"LLM fixes its own code errors. Runs the code, reads error messages, generates explanation of what went wrong, then corrects.",strengths:["+12% solve rate on MBPP","+2-3% on Spider SQL benchmark","+9% on hardest SQL queries"],weaknesses:["Requires execution environment","May enter infinite repair loops","Limited by model's debugging skills"],referenceUrl:"https://arxiv.org/abs/2304.05128",benchmarkResults:"Improved MBPP solve rate by 12 percentage points"},{id:"reflexion-code",name:"Reflexion / Iterative Prompting for Code",year:2023,category:"Code Generation & Reasoning",shortDescription:"Multiple attempts guided by self-reflection. Model tries, checks result, thinks aloud about errors, then attempts again.",strengths:["88% pass@1 on HumanEval","Near perfection on certain coding challenges","Eliminates trivial errors through iteration"],weaknesses:["Longer runtime (multiple model calls)","Careful prompt design required","May not overcome fundamental capability gaps"],referenceUrl:"https://arxiv.org/abs/2303.11366",benchmarkResults:"Reflexion-based agent reached 88% pass@1 on HumanEval"},{id:"code-cot",name:"Code Chain-of-Thought & Planning",year:2022,category:"Code Generation & Reasoning",shortDescription:"Reasoning steps in natural language before coding. Outlines solution approach or writes pseudo-code, then produces final code.",strengths:["Produces well-organized code","Reduces logical mistakes","Human-like planning for coding"],weaknesses:["Planning may not match execution needs","Adds overhead for simple tasks","Often combined with other methods"],referenceUrl:"https://arxiv.org/abs/2211.01910",benchmarkResults:"Improves success on multi-step coding puzzles when combined with Least-to-Most"},{id:"minerva",name:"CoT with Specialized Training (Minerva)",year:2022,category:"Mathematical Reasoning",shortDescription:"Training LLMs on step-by-step solutions. Fine-tuned on 118GB of math/science texts with CoT prompting + voting at inference.",strengths:["50.3% on MATH (vs previous 6.9%)","SOTA on GSM8K and STEM exams","Unlocks emergent math abilities"],weaknesses:["Needs large curated training data","Very large model sizes required","May not generalize to novel problem types"],referenceUrl:"https://research.google/blog/minerva-solving-quantitative-reasoning-problems-with-language-models/",benchmarkResults:"50.3% accuracy on MATH dataset vs previous best of 6.9%"},{id:"math-self-consistency",name:"Self-Consistency & Voting in Math",year:2022,category:"Mathematical Reasoning",shortDescription:"Mitigating calculation errors by consensus. Samples many CoT solutions and takes majority vote on final answer.",strengths:["+17.9% absolute accuracy on GSM8K","74% to 78.5% on GSM8K with voting","Boosts reliability without additional training"],weaknesses:["More inference passes required","Wrong answers may still converge","Doesn't improve underlying capabilities"],referenceUrl:"https://arxiv.org/abs/2203.11171",benchmarkResults:"Voting bumped PaLM 540B model's GSM8K score from ~74% to 78.5%"},{id:"pal-math",name:"Program-Aided Math Solving (PAL)",year:2022,category:"Mathematical Reasoning",shortDescription:"Language model writes a program to do the math. Outputs solvable program instead of numeric answer, defers to executor.",strengths:["48.2% to 58.4% on GSM8K","Introduces symbolic precision","Eliminates arithmetic errors"],weaknesses:["Requires runtime integration","May struggle if coding is incorrect","Problem must be code-expressible"],referenceUrl:"https://arxiv.org/abs/2211.10435",benchmarkResults:"Improved accuracy from 48.2% to 58.4% on GSM8K"},{id:"tot-puzzle",name:"Tree-of-Thought for Puzzle Solving",year:2023,category:"Mathematical Reasoning",shortDescription:"Systematic search for math puzzles. Explores different solution paths and backtracks when a path is invalid.",strengths:["74% on Game of 24 (vs <10% linear)","Dramatically boosts combinatorial problems","Ensures model doesn't get stuck"],weaknesses:["Complex control logic needed","More calls to the model","May not scale to very large search spaces"],referenceUrl:"https://arxiv.org/abs/2305.10601",benchmarkResults:"GPT-4 with ToT solved 74% vs <10% with linear reasoning"},{id:"least-to-most",name:"Least-to-Most Decomposition",year:2022,category:"Mathematical Reasoning",shortDescription:"Break complex problems into subproblems. Generates simpler sub-questions and solves them incrementally.",strengths:["Outperforms standard CoT on certain tasks","Reduces cognitive load per step","Mimics human problem-solving approach"],weaknesses:["Relies on correct subproblem generation","May miss holistic solution strategies","Can be tricky to decompose correctly"],referenceUrl:"https://arxiv.org/abs/2205.10625",benchmarkResults:"Higher accuracy than CoT on last letter concatenation puzzle"},{id:"cot-commonsense",name:"CoT + Self-Consistency (Commonsense)",year:2022,category:"Commonsense & Logical Reasoning",shortDescription:"Step-by-step reasoning for commonsense QA. Combines CoT with self-consistency for robust answers.",strengths:["+6.4% on StrategyQA","+3.9% on ARC-Challenge","Broadly applicable to logic puzzles"],weaknesses:["Smaller gains than in math","May not overcome knowledge gaps","Commonsense failures can be systematic"],referenceUrl:"https://arxiv.org/abs/2203.11171",benchmarkResults:"+6.4% on StrategyQA and +3.9% on ARC-Challenge"},{id:"decomposition",name:"Decomposition Prompting (Least-to-Most)",year:2022,category:"Commonsense & Logical Reasoning",shortDescription:"Explicitly break down complex questions. Generates intermediate questions and answers them sequentially.",strengths:["Higher accuracy than vanilla CoT","Handles multi-hop reasoning","Ensures each piece is handled"],weaknesses:["Decomposition errors propagate","Chain of subquestions can go astray","May lose context across sub-questions"],referenceUrl:"https://arxiv.org/abs/2205.10625",benchmarkResults:"Outperformed vanilla CoT on multi-step reasoning tasks"},{id:"ircot",name:"Iterative Retrieval + CoT (IRCoT)",year:2023,category:"Commonsense & Logical Reasoning",shortDescription:"Fetch relevant knowledge at each reasoning step. Interleaves retrieval with chain-of-thought reasoning.",strengths:["+15 points QA accuracy on HotpotQA","+21 points retrieval precision","Stays grounded in facts for multi-hop QA"],weaknesses:["Requires search API","Complex pipeline management","Retrieval latency adds up"],referenceUrl:"https://arxiv.org/abs/2212.10509",benchmarkResults:"Up to +15 points on QA accuracy and +21 on retrieval precision"},{id:"lot",name:"Logic-Augmented Prompting (LoT)",year:2024,category:"Commonsense & Logical Reasoning",shortDescription:"Inject formal logic structure into reasoning. Extracts propositional logic from text as additional context.",strengths:["+4.35% on ReClor (LSAT-like QA)","+5% on LogiQA with CoT+SC","+8% on ProofWriter with ToT"],weaknesses:["Needs reliable text-to-logic parsing","May not generalize to all problems","Added complexity in prompt design"],referenceUrl:"https://arxiv.org/abs/2401.04073",benchmarkResults:"+4.35% on ReClor, +5% on LogiQA, +8% on ProofWriter"},{id:"neuro-symbolic",name:"Hybrid Neuro-Symbolic Solvers",year:2023,category:"Commonsense & Logical Reasoning",shortDescription:"External logical tools with LLM guidance. LLMs call dedicated logic engines (e.g., SAT solvers).",strengths:["Perfect accuracy on certain puzzle tasks","Combines language understanding with symbolic rigor","Guaranteed logical reasoning"],weaknesses:["Complex system integration","Requires domain-specific solvers","Less common in general usage"],referenceUrl:"https://arxiv.org/abs/2304.09102",benchmarkResults:"Can achieve perfect accuracy on logic grid puzzles"},{id:"multimodal-cot",name:"Multimodal Chain-of-Thought",year:2022,category:"Multi-Modal Reasoning",shortDescription:"Reasoning chains that include visual inputs. Produces explanations that reference image content before answering.",strengths:["90.45% on ScienceQA (vs 86.54% prior)","~4% jump on visual reasoning tasks","Transparent decision process"],weaknesses:["Requires multimodal training","Model must understand text and images jointly","Visual reasoning steps can be imprecise"],referenceUrl:"https://arxiv.org/abs/2302.00923",benchmarkResults:"90.45% accuracy on ScienceQA, improving over prior SOTA of 86.54%"},{id:"palm-e",name:"Embodied Multimodal Models (PaLM-E)",year:2023,category:"Multi-Modal Reasoning",shortDescription:"Feeding visual features into giant language models. Connects 540B LM with vision encoders for multimodal reasoning.",strengths:["66.1% on OK-VQA (outperforms specialists)","Excels in captioning and planning","Seamless text and vision reasoning"],weaknesses:["Sheer model size and complexity","Expensive training requirements","Limited to specific embodied domains"],referenceUrl:"https://palm-e.github.io/",benchmarkResults:"66.1% on OK-VQA, outperforming specialist models"},{id:"tool-visual",name:"Tool-Augmented Visual Reasoning",year:2023,category:"Multi-Modal Reasoning",shortDescription:"Using external tools on visual tasks via reasoning. Allows VLMs to call OCR, calculators, or vision APIs.",strengths:["Addresses pure vision-to-text failures","Significantly improves chart QA accuracy","Modular and extensible"],weaknesses:["Pipeline complexity","Errors can compound across tools","Tool integration overhead"],referenceUrl:"https://arxiv.org/abs/2303.04671",benchmarkResults:"Solves problems no single model could solve alone on diagram reasoning"},{id:"vlm-cot",name:"Large VLMs with CoT (GPT-4V & others)",year:2023,category:"Multi-Modal Reasoning",shortDescription:"Very large multimodal models that inherently reason. Reasons through image content stepwise to answer queries.",strengths:["Exceeds human performance on VQAv2 metrics","Benefits from all text-domain advances","Unified architecture simplicity"],weaknesses:["Huge scale required","Difficulty evaluating reasoning vs pattern matching","May hallucinate visual details"],referenceUrl:"https://openai.com/research/gpt-4v-system-card",benchmarkResults:"GPT-4V exceeds human performance on some VQA metrics"}];var C=e.i(53199),M=e.i(88113),R=e.i(99002),z=e.i(62770);let T=[{id:"elite",title:"Elite Mode",description:"Industry-leading orchestration strategies",icon:j,color:"from-yellow-500 to-amber-600",isPremium:!0},{id:"models",title:"Models",description:"Select AI models for multi-agent orchestration",icon:d.Cpu,color:"from-orange-500 to-amber-500"},{id:"reasoning",title:"Reasoning",description:"Advanced reasoning methods and techniques",icon:m.Brain,color:"from-purple-500 to-indigo-500"},{id:"tuning",title:"Tuning",description:"Fine-tune response behavior and validation",icon:u.Sliders,color:"from-emerald-500 to-teal-500"},{id:"features",title:"Features",description:"RAG, MCP, memory and more capabilities",icon:p.Wrench,color:"from-blue-500 to-cyan-500"},{id:"tools",title:"Tools",description:"External integrations and tool access",icon:g,color:"from-rose-500 to-pink-500"},{id:"quality",title:"Quality",description:"Verification, challenge loops and fact-checking",icon:w.Shield,color:"from-green-500 to-emerald-600"},{id:"speed",title:"Speed",description:"Response speed and processing depth",icon:y.Zap,color:"from-cyan-500 to-sky-500"}],A=[{id:"fast",label:"Fast",description:"Quick single-model responses",confidence:"70%"},{id:"standard",label:"Standard",description:"Multi-model with verification",confidence:"80%"},{id:"thorough",label:"Thorough",description:"Full pipeline with challenge loop",confidence:"90%"},{id:"exhaustive",label:"Exhaustive",description:"All techniques including debate",confidence:"95%"}],L=[{id:"verification",label:"Fact Verification",description:"Verify all factual claims"},{id:"challenge",label:"Challenge Loop",description:"Adversarial stress-testing of answers"},{id:"consensus",label:"Multi-Model Consensus",description:"Agreement between multiple models"},{id:"reflection",label:"Self-Reflection",description:"Models critique and improve their output"},{id:"tools",label:"Tool Integration",description:"Use search, calculator, code execution"}],P=[{id:"vector-rag",label:"Vector DB + RAG",description:"Retrieval augmented generation with vector database"},{id:"mcp-server",label:"MCP Server + Tools",description:"Model context protocol for tool orchestration"},{id:"personal-database",label:"Personal Database",description:"Your private knowledge base"},{id:"modular-answer-feed",label:"Modular Answer Feed",description:"Internal LLM routing and composition"},{id:"memory-augmentation",label:"Memory Augmentation",description:"Long-term conversation memory"},{id:"code-interpreter",label:"Code Interpreter",description:"Execute code in a sandboxed environment"}],O=[{id:"web-search",label:"Web Search",description:"Search the internet for real-time information"},{id:"code-execution",label:"Code Execution",description:"Run Python, JavaScript, and more"},{id:"file-analysis",label:"File Analysis",description:"Parse and analyze documents"},{id:"image-generation",label:"Image Generation",description:"Create images from text"},{id:"data-visualization",label:"Data Visualization",description:"Generate charts and graphs"},{id:"api-integration",label:"API Integration",description:"Connect to external APIs"}],G=[{key:"promptOptimization",label:"Prompt Optimization",description:"Automatically enhance prompts",icon:v.Sparkles},{key:"outputValidation",label:"Output Validation",description:"Verify and fact-check responses",icon:b.Check},{key:"answerStructure",label:"Answer Structure",description:"Format with clear sections",icon:x.Layers},{key:"sharedMemory",label:"Shared Memory",description:"Access previous conversations",icon:g},{key:"learnFromChat",label:"Learn from Chat",description:"Improve from this conversation",icon:p.Wrench}],E=[{id:"temperature",label:"Temperature",description:"Controls randomness (0-2)",min:0,max:2,step:.1,default:.7},{id:"maxTokens",label:"Max Tokens",description:"Maximum response length",min:100,max:4e3,step:100,default:2e3},{id:"topP",label:"Top P",description:"Nucleus sampling threshold",min:0,max:1,step:.05,default:.9},{id:"frequencyPenalty",label:"Frequency Penalty",description:"Reduce repetition",min:0,max:2,step:.1,default:0},{id:"presencePenalty",label:"Presence Penalty",description:"Encourage new topics",min:0,max:2,step:.1,default:0}],q=[{id:"fast",label:"Fast",description:"Quick responses with minimal processing"},{id:"standard",label:"Standard",description:"Balanced speed and quality"},{id:"deep",label:"Deep",description:"Thorough analysis with extended processing"}];function D(){let e=(0,t.useRouter)(),[h,v]=(0,s.useState)(null),[D,H]=(0,s.useState)(!1),[U,V]=(0,s.useState)(null),[I,F]=(0,s.useState)(!1),[Q,B]=(0,s.useState)(["automatic"]),[$,K]=(0,s.useState)([]),[W,_]=(0,s.useState)([]),[Z,J]=(0,s.useState)([]),[Y,X]=(0,s.useState)({promptOptimization:!0,outputValidation:!0,answerStructure:!1,sharedMemory:!0,learnFromChat:!1}),[ee,er]=(0,s.useState)({temperature:.7,maxTokens:2e3,topP:.9,frequencyPenalty:0,presencePenalty:0}),[es,et]=(0,s.useState)("standard"),[ea,en]=(0,s.useState)("standard"),[eo,ei]=(0,s.useState)(["verification","consensus"]);return(0,s.useEffect)(()=>{let e=(0,R.loadOrchestratorSettings)();B(e.selectedModels||["automatic"]),K(e.advancedReasoningMethods||[]),_(e.advancedFeatures||[]),J(e.advancedFeatures?.filter(e=>["web-search","code-execution","file-analysis","image-generation","data-visualization","api-integration"].includes(e))||[]),X({promptOptimization:e.promptOptimization,outputValidation:e.outputValidation,answerStructure:e.answerStructure,sharedMemory:e.sharedMemory,learnFromChat:e.learnFromChat}),et(e.reasoningMode||"standard"),en(e.eliteStrategy||"standard"),ei(e.qualityOptions||["verification","consensus"]),e.standardValues&&er(e.standardValues),F(!0)},[]),(0,s.useEffect)(()=>{I&&(0,R.saveOrchestratorSettings)({selectedModels:Q,advancedReasoningMethods:$,advancedFeatures:[...W,...Z],promptOptimization:Y.promptOptimization,outputValidation:Y.outputValidation,answerStructure:Y.answerStructure,sharedMemory:Y.sharedMemory,learnFromChat:Y.learnFromChat,reasoningMode:es,eliteStrategy:ea,qualityOptions:eo,standardValues:ee})},[Q,$,W,Z,Y,es,ea,eo,ee,I]),(0,r.jsxs)("div",{className:"flex h-screen bg-background",children:[(0,r.jsx)(C.Sidebar,{collapsed:D,onToggleCollapse:()=>H(!D),conversations:[],currentConversationId:null,onSelectConversation:()=>e.push(z.ROUTES.HOME),onNewChat:()=>e.push(z.ROUTES.HOME),onDeleteConversation:()=>{},onTogglePin:()=>{},onRenameConversation:()=>{},onMoveToProject:()=>{},projects:[],onGoHome:()=>e.push(z.ROUTES.HOME)}),(0,r.jsxs)("div",{className:"flex-1 flex flex-col min-h-0 overflow-hidden",children:[(0,r.jsx)("div",{className:"hidden md:flex items-center justify-end p-3 border-b border-border bg-card/50",children:(0,r.jsx)(M.UserAccountMenu,{user:U,onSignIn:()=>{},onSignOut:()=>V(null)})}),(0,r.jsx)("div",{className:"flex-1 h-full overflow-auto",children:(0,r.jsxs)("div",{className:"min-h-full flex flex-col items-center justify-start px-4 pt-0 pb-20",children:[(0,r.jsxs)("div",{className:"text-center mb-0",children:[(0,r.jsx)("div",{className:"relative w-40 h-40 md:w-[280px] md:h-[280px] lg:w-[320px] lg:h-[320px] mx-auto mb-0 -mt-4 md:-mt-8 lg:-mt-10",children:(0,r.jsx)(a.default,{src:"/logo.png",alt:"LLMHive",fill:!0,className:"object-contain",priority:!0})}),(0,r.jsx)("h1",{className:"-mt-6 md:-mt-8 lg:-mt-10 text-[1.75rem] md:text-[2.85rem] lg:text-[3.4rem] font-bold mb-1 bg-gradient-to-r from-[var(--bronze)] via-[var(--gold)] to-[var(--bronze)] bg-clip-text text-transparent",children:"Orchestration"}),(0,r.jsx)("p",{className:"text-muted-foreground text-sm md:text-base max-w-md mx-auto mb-0",children:"Configure your multi-agent AI system with precision"})]}),(0,r.jsx)("div",{className:"w-16 h-px bg-border my-2"}),(0,r.jsxs)("div",{className:"w-full max-w-5xl",children:[(0,r.jsx)("p",{className:"text-sm text-muted-foreground text-center mb-2",children:"Select a category to configure"}),(0,r.jsx)("div",{className:"grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 xl:grid-cols-6 gap-3",children:T.map(e=>{let s=e.icon,t=(e=>{switch(e){case"models":return Q.length;case"reasoning":return $.length;case"features":return W.length;case"tools":return Z.length;case"tuning":return Object.values(Y).filter(Boolean).length;case"speed":case"elite":return 1;case"quality":return eo.length;default:return 0}})(e.id);return(0,r.jsxs)("button",{onClick:()=>v(e.id),className:"group flex flex-col items-center gap-2 p-3 md:p-4 rounded-xl border border-border hover:border-[var(--bronze)] bg-card/50 hover:bg-card/80 transition-all duration-300 cursor-pointer text-left relative",children:[t>0&&(0,r.jsx)(i.Badge,{className:"absolute top-2 right-2 bg-[var(--bronze)] text-black text-[10px] px-1.5 py-0.5 min-w-[18px] h-[18px] flex items-center justify-center",children:t}),(0,r.jsx)("div",{className:`w-10 h-10 md:w-12 md:h-12 rounded-xl bg-gradient-to-br ${e.color} flex items-center justify-center shadow-lg group-hover:scale-110 group-hover:shadow-xl transition-all duration-300`,children:(0,r.jsx)(s,{className:"h-5 w-5 md:h-6 md:w-6 text-white"})}),(0,r.jsxs)("div",{className:"text-center",children:[(0,r.jsx)("h3",{className:"text-sm md:text-base font-semibold text-foreground group-hover:text-[var(--bronze)] transition-colors",children:e.title}),(0,r.jsx)("p",{className:"text-xs text-muted-foreground mt-1 line-clamp-2",children:e.description})]})]},e.id)})})]})]})})]}),(0,r.jsx)(c.Sheet,{open:"models"===h,onOpenChange:e=>!e&&v(null),children:(0,r.jsxs)(c.SheetContent,{side:"right",className:"w-[260px] sm:w-[280px] bg-background/95 backdrop-blur-xl border-l border-border/50 p-0",children:[(0,r.jsx)("div",{className:"p-5 border-b border-border/50",children:(0,r.jsxs)(c.SheetHeader,{className:"space-y-1",children:[(0,r.jsxs)(c.SheetTitle,{className:"text-base font-medium flex items-center gap-2",children:[(0,r.jsx)("div",{className:"w-8 h-8 rounded-lg bg-gradient-to-br from-blue-500/20 to-cyan-500/20 flex items-center justify-center",children:(0,r.jsx)(d.Cpu,{className:"h-4 w-4 text-blue-400"})}),"Models",(0,r.jsxs)("span",{className:"ml-auto text-xs font-normal text-[var(--bronze)] bg-[var(--bronze)]/10 px-2 py-0.5 rounded-full",children:[Q.length," selected"]})]}),(0,r.jsx)("p",{className:"text-xs text-muted-foreground",children:"Select AI models for orchestration"})]})}),(0,r.jsx)(l.ScrollArea,{className:"h-[calc(100vh-100px)]",children:(0,r.jsx)("div",{className:"p-4 space-y-1.5",children:N.AVAILABLE_MODELS.map(e=>{let s=Q.includes(e.id);return(0,r.jsx)("div",{onClick:()=>{var r;return r=e.id,void B(e=>e.includes(r)?e.filter(e=>e!==r):[...e,r])},className:`group p-2.5 rounded-lg cursor-pointer transition-all duration-200 ${s?"bg-[var(--bronze)]/10 ring-1 ring-[var(--bronze)]/30":"hover:bg-muted/50"}`,children:(0,r.jsxs)("div",{className:"flex items-center gap-2.5",children:[(0,r.jsx)("div",{className:`w-4 h-4 rounded border-2 flex items-center justify-center transition-all flex-shrink-0 ${s?"border-[var(--bronze)] bg-[var(--bronze)]":"border-muted-foreground/30 group-hover:border-muted-foreground/50"}`,children:s&&(0,r.jsx)(b.Check,{className:"h-2.5 w-2.5 text-background"})}),(0,r.jsx)("div",{className:"w-6 h-6 relative flex-shrink-0 rounded-md overflow-hidden bg-muted/50",children:(0,r.jsx)(a.default,{src:(0,N.getModelLogo)(e.provider)||"/placeholder.svg",alt:e.provider,fill:!0,className:"object-contain p-0.5"})}),(0,r.jsxs)("div",{className:"flex-1 min-w-0",children:[(0,r.jsx)("p",{className:`text-sm font-medium truncate ${s?"text-[var(--bronze)]":""}`,children:e.name}),(0,r.jsx)("p",{className:"text-[10px] text-muted-foreground capitalize",children:e.provider})]})]})},e.id)})})})]})}),(0,r.jsx)(c.Sheet,{open:"reasoning"===h,onOpenChange:e=>!e&&v(null),children:(0,r.jsxs)(c.SheetContent,{side:"right",className:"w-[260px] sm:w-[280px] bg-background/95 backdrop-blur-xl border-l border-border/50 p-0",children:[(0,r.jsx)("div",{className:"p-5 border-b border-border/50",children:(0,r.jsxs)(c.SheetHeader,{className:"space-y-1",children:[(0,r.jsxs)(c.SheetTitle,{className:"text-base font-medium flex items-center gap-2",children:[(0,r.jsx)("div",{className:"w-8 h-8 rounded-lg bg-gradient-to-br from-purple-500/20 to-pink-500/20 flex items-center justify-center",children:(0,r.jsx)(m.Brain,{className:"h-4 w-4 text-purple-400"})}),"Reasoning",(0,r.jsxs)("span",{className:"ml-auto text-xs font-normal text-[var(--bronze)] bg-[var(--bronze)]/10 px-2 py-0.5 rounded-full",children:[$.length," selected"]})]}),(0,r.jsx)("p",{className:"text-xs text-muted-foreground",children:"Advanced reasoning methods"})]})}),(0,r.jsx)(l.ScrollArea,{className:"h-[calc(100vh-100px)]",children:(0,r.jsx)("div",{className:"p-4",children:k.map(e=>{let s=S.filter(r=>r.category===e);return(0,r.jsxs)("div",{className:"mb-5",children:[(0,r.jsx)("h3",{className:"text-[10px] font-semibold uppercase tracking-wider text-muted-foreground mb-2 px-1",children:e}),(0,r.jsx)("div",{className:"space-y-1.5",children:s.map(e=>{let s=$.includes(e.id);return(0,r.jsx)("div",{onClick:()=>{var r;return r=e.id,void K(e=>e.includes(r)?e.filter(e=>e!==r):[...e,r])},className:`group p-2.5 rounded-lg cursor-pointer transition-all duration-200 ${s?"bg-[var(--bronze)]/10 ring-1 ring-[var(--bronze)]/30":"hover:bg-muted/50"}`,children:(0,r.jsxs)("div",{className:"flex items-center gap-2.5",children:[(0,r.jsx)("div",{className:`w-4 h-4 rounded border-2 flex items-center justify-center transition-all flex-shrink-0 ${s?"border-[var(--bronze)] bg-[var(--bronze)]":"border-muted-foreground/30 group-hover:border-muted-foreground/50"}`,children:s&&(0,r.jsx)(b.Check,{className:"h-2.5 w-2.5 text-background"})}),(0,r.jsx)("div",{className:"flex-1 min-w-0",children:(0,r.jsxs)("div",{className:"flex items-center gap-1.5",children:[(0,r.jsx)("p",{className:"text-sm font-medium truncate",children:e.name}),(0,r.jsx)("span",{className:"text-[9px] text-muted-foreground bg-muted/50 px-1 rounded",children:e.year})]})}),(0,r.jsx)("a",{href:e.referenceUrl,target:"_blank",rel:"noopener noreferrer",className:"opacity-0 group-hover:opacity-100 transition-opacity",onClick:e=>e.stopPropagation(),children:(0,r.jsx)(f.ExternalLink,{className:"h-3 w-3 text-muted-foreground hover:text-[var(--bronze)]"})})]})},e.id)})})]},e)})})})]})}),(0,r.jsx)(c.Sheet,{open:"tuning"===h,onOpenChange:e=>!e&&v(null),children:(0,r.jsxs)(c.SheetContent,{side:"right",className:"w-[260px] sm:w-[280px] bg-background/95 backdrop-blur-xl border-l border-border/50 p-0",children:[(0,r.jsx)("div",{className:"p-5 border-b border-border/50",children:(0,r.jsxs)(c.SheetHeader,{className:"space-y-1",children:[(0,r.jsxs)(c.SheetTitle,{className:"text-base font-medium flex items-center gap-2",children:[(0,r.jsx)("div",{className:"w-8 h-8 rounded-lg bg-gradient-to-br from-amber-500/20 to-orange-500/20 flex items-center justify-center",children:(0,r.jsx)(u.Sliders,{className:"h-4 w-4 text-amber-400"})}),"Tuning",(0,r.jsxs)("span",{className:"ml-auto text-xs font-normal text-[var(--bronze)] bg-[var(--bronze)]/10 px-2 py-0.5 rounded-full",children:[Object.values(Y).filter(Boolean).length," enabled"]})]}),(0,r.jsx)("p",{className:"text-xs text-muted-foreground",children:"Fine-tune response behavior"})]})}),(0,r.jsx)(l.ScrollArea,{className:"h-[calc(100vh-100px)]",children:(0,r.jsx)("div",{className:"p-4 space-y-1.5",children:G.map(e=>{let s=Y[e.key];return(0,r.jsx)("div",{onClick:()=>X(r=>({...r,[e.key]:!r[e.key]})),className:`group p-2.5 rounded-lg cursor-pointer transition-all duration-200 ${s?"bg-[var(--bronze)]/10 ring-1 ring-[var(--bronze)]/30":"hover:bg-muted/50"}`,children:(0,r.jsxs)("div",{className:"flex items-center gap-2.5",children:[(0,r.jsx)("div",{className:`w-4 h-4 rounded border-2 flex items-center justify-center transition-all flex-shrink-0 ${s?"border-[var(--bronze)] bg-[var(--bronze)]":"border-muted-foreground/30 group-hover:border-muted-foreground/50"}`,children:s&&(0,r.jsx)(b.Check,{className:"h-2.5 w-2.5 text-background"})}),(0,r.jsxs)("div",{className:"flex-1 min-w-0",children:[(0,r.jsx)("p",{className:`text-sm font-medium ${s?"text-[var(--bronze)]":""}`,children:e.label}),(0,r.jsx)("p",{className:"text-[10px] text-muted-foreground leading-tight",children:e.description})]})]})},e.key)})})})]})}),(0,r.jsx)(c.Sheet,{open:"features"===h,onOpenChange:e=>!e&&v(null),children:(0,r.jsxs)(c.SheetContent,{side:"right",className:"w-[260px] sm:w-[280px] bg-background/95 backdrop-blur-xl border-l border-border/50 p-0",children:[(0,r.jsx)("div",{className:"p-5 border-b border-border/50",children:(0,r.jsxs)(c.SheetHeader,{className:"space-y-1",children:[(0,r.jsxs)(c.SheetTitle,{className:"text-base font-medium flex items-center gap-2",children:[(0,r.jsx)("div",{className:"w-8 h-8 rounded-lg bg-gradient-to-br from-green-500/20 to-emerald-500/20 flex items-center justify-center",children:(0,r.jsx)(p.Wrench,{className:"h-4 w-4 text-green-400"})}),"Features",(0,r.jsxs)("span",{className:"ml-auto text-xs font-normal text-[var(--bronze)] bg-[var(--bronze)]/10 px-2 py-0.5 rounded-full",children:[W.length," enabled"]})]}),(0,r.jsx)("p",{className:"text-xs text-muted-foreground",children:"Advanced capabilities"})]})}),(0,r.jsx)(l.ScrollArea,{className:"h-[calc(100vh-100px)]",children:(0,r.jsx)("div",{className:"p-4 space-y-1.5",children:P.map(e=>{let s=W.includes(e.id);return(0,r.jsx)("div",{onClick:()=>{var r;return r=e.id,void _(e=>e.includes(r)?e.filter(e=>e!==r):[...e,r])},className:`group p-2.5 rounded-lg cursor-pointer transition-all duration-200 ${s?"bg-[var(--bronze)]/10 ring-1 ring-[var(--bronze)]/30":"hover:bg-muted/50"}`,children:(0,r.jsxs)("div",{className:"flex items-center gap-2.5",children:[(0,r.jsx)("div",{className:`w-4 h-4 rounded border-2 flex items-center justify-center transition-all flex-shrink-0 ${s?"border-[var(--bronze)] bg-[var(--bronze)]":"border-muted-foreground/30 group-hover:border-muted-foreground/50"}`,children:s&&(0,r.jsx)(b.Check,{className:"h-2.5 w-2.5 text-background"})}),(0,r.jsxs)("div",{className:"flex-1 min-w-0",children:[(0,r.jsx)("p",{className:`text-sm font-medium ${s?"text-[var(--bronze)]":""}`,children:e.label}),(0,r.jsx)("p",{className:"text-[10px] text-muted-foreground leading-tight",children:e.description})]})]})},e.id)})})})]})}),(0,r.jsx)(c.Sheet,{open:"tools"===h,onOpenChange:e=>!e&&v(null),children:(0,r.jsxs)(c.SheetContent,{side:"right",className:"w-[260px] sm:w-[280px] bg-background/95 backdrop-blur-xl border-l border-border/50 p-0",children:[(0,r.jsx)("div",{className:"p-5 border-b border-border/50",children:(0,r.jsxs)(c.SheetHeader,{className:"space-y-1",children:[(0,r.jsxs)(c.SheetTitle,{className:"text-base font-medium flex items-center gap-2",children:[(0,r.jsx)("div",{className:"w-8 h-8 rounded-lg bg-gradient-to-br from-rose-500/20 to-red-500/20 flex items-center justify-center",children:(0,r.jsx)(g,{className:"h-4 w-4 text-rose-400"})}),"Tools",(0,r.jsxs)("span",{className:"ml-auto text-xs font-normal text-[var(--bronze)] bg-[var(--bronze)]/10 px-2 py-0.5 rounded-full",children:[Z.length," enabled"]})]}),(0,r.jsx)("p",{className:"text-xs text-muted-foreground",children:"External integrations"})]})}),(0,r.jsx)(l.ScrollArea,{className:"h-[calc(100vh-100px)]",children:(0,r.jsx)("div",{className:"p-4 space-y-1.5",children:O.map(e=>{let s=Z.includes(e.id);return(0,r.jsx)("div",{onClick:()=>{var r;return r=e.id,void J(e=>e.includes(r)?e.filter(e=>e!==r):[...e,r])},className:`group p-2.5 rounded-lg cursor-pointer transition-all duration-200 ${s?"bg-[var(--bronze)]/10 ring-1 ring-[var(--bronze)]/30":"hover:bg-muted/50"}`,children:(0,r.jsxs)("div",{className:"flex items-center gap-2.5",children:[(0,r.jsx)("div",{className:`w-4 h-4 rounded border-2 flex items-center justify-center transition-all flex-shrink-0 ${s?"border-[var(--bronze)] bg-[var(--bronze)]":"border-muted-foreground/30 group-hover:border-muted-foreground/50"}`,children:s&&(0,r.jsx)(b.Check,{className:"h-2.5 w-2.5 text-background"})}),(0,r.jsxs)("div",{className:"flex-1 min-w-0",children:[(0,r.jsx)("p",{className:`text-sm font-medium ${s?"text-[var(--bronze)]":""}`,children:e.label}),(0,r.jsx)("p",{className:"text-[10px] text-muted-foreground leading-tight",children:e.description})]})]})},e.id)})})})]})}),(0,r.jsx)(c.Sheet,{open:"standard"===h,onOpenChange:e=>!e&&v(null),children:(0,r.jsxs)(c.SheetContent,{side:"right",className:"w-[260px] sm:w-[280px] bg-background/95 backdrop-blur-xl border-l border-border/50 p-0",children:[(0,r.jsx)("div",{className:"p-4 border-b border-border/50",children:(0,r.jsxs)(c.SheetHeader,{className:"space-y-1",children:[(0,r.jsxs)(c.SheetTitle,{className:"text-sm font-medium flex items-center gap-2",children:[(0,r.jsx)("div",{className:"w-6 h-6 rounded-md bg-gradient-to-br from-indigo-500/20 to-violet-500/20 flex items-center justify-center",children:(0,r.jsx)(x.Layers,{className:"h-3 w-3 text-indigo-400"})}),"Standard"]}),(0,r.jsx)("p",{className:"text-[10px] text-muted-foreground",children:"Temperature, tokens & sampling"})]})}),(0,r.jsx)("div",{className:"p-4 space-y-4",children:E.map(e=>(0,r.jsxs)("div",{className:"space-y-2",children:[(0,r.jsxs)("div",{className:"flex items-center justify-between",children:[(0,r.jsx)(n.Label,{className:"text-xs font-medium",children:e.label}),(0,r.jsx)("span",{className:"text-[10px] font-mono text-[var(--bronze)] bg-[var(--bronze)]/10 px-1.5 py-0.5 rounded",children:ee[e.id]})]}),(0,r.jsx)(o.Slider,{value:[ee[e.id]],onValueChange:([r])=>er(s=>({...s,[e.id]:r})),min:e.min,max:e.max,step:e.step,className:"[&_[role=slider]]:bg-[var(--bronze)] [&_[role=slider]]:border-[var(--bronze)] [&_[role=slider]]:w-3 [&_[role=slider]]:h-3 [&_.bg-primary]:bg-[var(--bronze)]"}),(0,r.jsx)("p",{className:"text-[9px] text-muted-foreground",children:e.description})]},e.id))})]})}),(0,r.jsx)(c.Sheet,{open:"speed"===h,onOpenChange:e=>!e&&v(null),children:(0,r.jsxs)(c.SheetContent,{side:"right",className:"w-[260px] sm:w-[280px] bg-background/95 backdrop-blur-xl border-l border-border/50 p-0",children:[(0,r.jsx)("div",{className:"p-4 border-b border-border/50",children:(0,r.jsxs)(c.SheetHeader,{className:"space-y-1",children:[(0,r.jsxs)(c.SheetTitle,{className:"text-sm font-medium flex items-center gap-2",children:[(0,r.jsx)("div",{className:"w-6 h-6 rounded-md bg-gradient-to-br from-cyan-500/20 to-sky-500/20 flex items-center justify-center",children:(0,r.jsx)(y.Zap,{className:"h-3 w-3 text-cyan-400"})}),"Speed"]}),(0,r.jsx)("p",{className:"text-[10px] text-muted-foreground",children:"Response speed & processing depth"})]})}),(0,r.jsx)(l.ScrollArea,{className:"h-[calc(100vh-120px)]",children:(0,r.jsx)("div",{className:"p-3 space-y-1.5",children:q.map(e=>{let s=es===e.id;return(0,r.jsxs)("button",{onClick:()=>et(e.id),className:`w-full flex items-center gap-2.5 p-2.5 rounded-lg transition-all duration-200 ${s?"bg-[var(--bronze)]/10 ring-1 ring-[var(--bronze)]/30":"hover:bg-muted/50"}`,children:[(0,r.jsx)("div",{className:`w-4 h-4 rounded border-2 flex items-center justify-center transition-all ${s?"bg-[var(--bronze)] border-[var(--bronze)]":"border-muted-foreground/30"}`,children:s&&(0,r.jsx)(b.Check,{className:"h-2.5 w-2.5 text-black"})}),(0,r.jsxs)("div",{className:"flex-1 text-left",children:[(0,r.jsx)("p",{className:`text-xs font-medium ${s?"text-[var(--bronze)]":"text-foreground"}`,children:e.label}),(0,r.jsx)("p",{className:"text-[10px] text-muted-foreground",children:e.description})]})]},e.id)})})})]})}),(0,r.jsx)(c.Sheet,{open:"elite"===h,onOpenChange:e=>!e&&v(null),children:(0,r.jsxs)(c.SheetContent,{side:"right",className:"w-[260px] sm:w-[280px] bg-background/95 backdrop-blur-xl border-l border-border/50 p-0",children:[(0,r.jsx)("div",{className:"p-5 border-b border-border/50",children:(0,r.jsxs)(c.SheetHeader,{className:"space-y-1",children:[(0,r.jsxs)(c.SheetTitle,{className:"text-base font-medium flex items-center gap-2",children:[(0,r.jsx)("div",{className:"w-8 h-8 rounded-lg bg-gradient-to-br from-yellow-500/20 to-amber-500/20 flex items-center justify-center",children:(0,r.jsx)(j,{className:"h-4 w-4 text-yellow-400"})}),"Elite Mode",(0,r.jsx)(i.Badge,{className:"ml-auto bg-gradient-to-r from-yellow-500 to-amber-600 text-white text-[10px]",children:"PRO"})]}),(0,r.jsx)("p",{className:"text-xs text-muted-foreground",children:"Industry-leading orchestration strategies"})]})}),(0,r.jsx)(l.ScrollArea,{className:"h-[calc(100vh-120px)]",children:(0,r.jsxs)("div",{className:"p-4 space-y-3",children:[(0,r.jsx)("p",{className:"text-[10px] uppercase tracking-wider text-muted-foreground font-semibold",children:"Orchestration Strategy"}),A.map(e=>{let s=ea===e.id;return(0,r.jsxs)("button",{onClick:()=>en(e.id),className:`w-full p-3 rounded-lg transition-all duration-200 text-left ${s?"bg-[var(--bronze)]/10 ring-1 ring-[var(--bronze)]/30":"hover:bg-muted/50 border border-border"}`,children:[(0,r.jsxs)("div",{className:"flex items-center justify-between mb-1",children:[(0,r.jsxs)("div",{className:"flex items-center gap-2",children:[(0,r.jsx)("div",{className:`w-4 h-4 rounded-full border-2 flex items-center justify-center transition-all ${s?"bg-[var(--bronze)] border-[var(--bronze)]":"border-muted-foreground/30"}`,children:s&&(0,r.jsx)(b.Check,{className:"h-2.5 w-2.5 text-black"})}),(0,r.jsx)("span",{className:`text-sm font-medium ${s?"text-[var(--bronze)]":""}`,children:e.label})]}),(0,r.jsx)(i.Badge,{variant:"secondary",className:"text-[9px] bg-green-500/10 text-green-400",children:e.confidence})]}),(0,r.jsx)("p",{className:"text-[10px] text-muted-foreground ml-6",children:e.description})]},e.id)}),(0,r.jsxs)("div",{className:"pt-4 border-t border-border/50 mt-4",children:[(0,r.jsx)("p",{className:"text-[10px] uppercase tracking-wider text-muted-foreground font-semibold mb-3",children:"How It Works"}),(0,r.jsxs)("div",{className:"space-y-2 text-[10px] text-muted-foreground",children:[(0,r.jsxs)("p",{children:["• ",(0,r.jsx)("strong",{children:"Fast:"})," Single best model, quick responses"]}),(0,r.jsxs)("p",{children:["• ",(0,r.jsx)("strong",{children:"Standard:"})," Multi-model with quality fusion"]}),(0,r.jsxs)("p",{children:["• ",(0,r.jsx)("strong",{children:"Thorough:"})," Challenge loop stress-tests answers"]}),(0,r.jsxs)("p",{children:["• ",(0,r.jsx)("strong",{children:"Exhaustive:"})," Expert panel + debate + verification"]})]})]})]})})]})}),(0,r.jsx)(c.Sheet,{open:"quality"===h,onOpenChange:e=>!e&&v(null),children:(0,r.jsxs)(c.SheetContent,{side:"right",className:"w-[260px] sm:w-[280px] bg-background/95 backdrop-blur-xl border-l border-border/50 p-0",children:[(0,r.jsx)("div",{className:"p-5 border-b border-border/50",children:(0,r.jsxs)(c.SheetHeader,{className:"space-y-1",children:[(0,r.jsxs)(c.SheetTitle,{className:"text-base font-medium flex items-center gap-2",children:[(0,r.jsx)("div",{className:"w-8 h-8 rounded-lg bg-gradient-to-br from-green-500/20 to-emerald-500/20 flex items-center justify-center",children:(0,r.jsx)(w.Shield,{className:"h-4 w-4 text-green-400"})}),"Quality",(0,r.jsxs)("span",{className:"ml-auto text-xs font-normal text-[var(--bronze)] bg-[var(--bronze)]/10 px-2 py-0.5 rounded-full",children:[eo.length," enabled"]})]}),(0,r.jsx)("p",{className:"text-xs text-muted-foreground",children:"Verification and quality assurance"})]})}),(0,r.jsx)(l.ScrollArea,{className:"h-[calc(100vh-100px)]",children:(0,r.jsx)("div",{className:"p-4 space-y-1.5",children:L.map(e=>{let s=eo.includes(e.id);return(0,r.jsx)("div",{onClick:()=>{var r;return r=e.id,void ei(e=>e.includes(r)?e.filter(e=>e!==r):[...e,r])},className:`group p-2.5 rounded-lg cursor-pointer transition-all duration-200 ${s?"bg-[var(--bronze)]/10 ring-1 ring-[var(--bronze)]/30":"hover:bg-muted/50"}`,children:(0,r.jsxs)("div",{className:"flex items-center gap-2.5",children:[(0,r.jsx)("div",{className:`w-4 h-4 rounded border-2 flex items-center justify-center transition-all flex-shrink-0 ${s?"border-[var(--bronze)] bg-[var(--bronze)]":"border-muted-foreground/30 group-hover:border-muted-foreground/50"}`,children:s&&(0,r.jsx)(b.Check,{className:"h-2.5 w-2.5 text-background"})}),(0,r.jsxs)("div",{className:"flex-1 min-w-0",children:[(0,r.jsx)("p",{className:`text-sm font-medium ${s?"text-[var(--bronze)]":""}`,children:e.label}),(0,r.jsx)("p",{className:"text-[10px] text-muted-foreground leading-tight",children:e.description})]})]})},e.id)})})})]})})]})}e.s(["default",()=>D],66291)}]);