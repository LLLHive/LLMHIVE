module.exports=[23289,a=>{"use strict";var b=a.i(87924),c=a.i(72131),d=a.i(50944),e=a.i(71987),f=a.i(17171),g=a.i(44010),h=a.i(54161),i=a.i(46470),j=a.i(53071),k=a.i(94392),l=a.i(61545),m=a.i(36526),n=a.i(79808),o=a.i(70106);let p=(0,o.default)("Hammer",[["path",{d:"m15 12-8.373 8.373a1 1 0 1 1-3-3L12 9",key:"eefl8a"}],["path",{d:"m18 15 4-4",key:"16gjal"}],["path",{d:"m21.5 11.5-1.914-1.914A2 2 0 0 1 19 8.172V7l-2.26-2.26a6 6 0 0 0-4.202-1.756L9 2.96l.92.82A6.18 6.18 0 0 1 12 8.4V10l2 2h1.172a2 2 0 0 1 1.414.586L18.5 14.5",key:"b7pghm"}]]);var q=a.i(17545),r=a.i(33441),s=a.i(52495),t=a.i(8406),u=a.i(1027);let v=(0,o.default)("Crown",[["path",{d:"M11.562 3.266a.5.5 0 0 1 .876 0L15.39 8.87a1 1 0 0 0 1.516.294L21.183 5.5a.5.5 0 0 1 .798.519l-2.834 10.246a1 1 0 0 1-.956.734H5.81a1 1 0 0 1-.957-.734L2.02 6.02a.5.5 0 0 1 .798-.519l4.276 3.664a1 1 0 0 0 1.516-.294z",key:"1vdc57"}],["path",{d:"M5 21h14",key:"11awu3"}]]);var w=a.i(3314),x=a.i(8720);let y=["Automatic","General Reasoning","Code Generation & Reasoning","Mathematical Reasoning","Commonsense & Logical Reasoning","Multi-Modal Reasoning"],z=[{id:"automatic",name:"Automatic",year:2025,category:"Automatic",shortDescription:"Let the LLMHive orchestrator automatically select the optimal reasoning method based on your query.",strengths:["Optimal method selection for each task","No configuration required","Adaptive to query complexity","Best overall performance"],weaknesses:["Less control over specific method used"],referenceUrl:"https://llmhive.ai/docs/automatic-reasoning",benchmarkResults:"Achieves best average performance across all benchmarks by dynamically selecting optimal methods"},{id:"cot",name:"Chain-of-Thought (CoT) Prompting",year:2022,category:"General Reasoning",shortDescription:"Breaks problems into steps via prompt engineering. Enables LMs to generate intermediate reasoning steps before the final answer.",strengths:["Improves accuracy on multi-step reasoning tasks","State-of-the-art on GSM8K math benchmark","Works across domains (math, logic, commonsense)"],weaknesses:["Increases token usage and latency","Mainly effective in sufficiently large models","May produce verbose reasoning"],referenceUrl:"https://arxiv.org/abs/2201.11903",benchmarkResults:"540B model with CoT achieved SOTA on GSM8K, surpassing fine-tuned GPT-3"},{id:"self-consistency",name:"Self-Consistency Decoding",year:2022,category:"General Reasoning",shortDescription:"Majority-vote over multiple reasoning paths. Samples diverse CoT trails and chooses the most common answer.",strengths:["+17.9% accuracy on GSM8K","+11% on SVAMP, +12% on AQuA","+6.4% on StrategyQA, +3.9% on ARC-Challenge"],weaknesses:["Higher compute cost (sampling many outputs)","Requires multiple inference passes","May not help if all paths share the same bias"],referenceUrl:"https://arxiv.org/abs/2203.11171",benchmarkResults:"Improved CoT performance on GSM8K by +17.9% absolute"},{id:"tree-of-thoughts",name:"Tree-of-Thoughts (ToT) Search",year:2023,category:"General Reasoning",shortDescription:"Deliberative branching of reasoning steps. Explores a tree of possible thought sequences and backtracks as needed.",strengths:["74% solve rate on Game of 24 (vs 4% with CoT)","Excels at planning and multi-hop reasoning","Systematically finds correct solutions"],weaknesses:["Extra prompting steps and state management","High computational overhead","Complex to implement and tune"],referenceUrl:"https://arxiv.org/abs/2305.10601",benchmarkResults:"GPT-4 with ToT solved 74% of Game of 24 challenges vs ~4% with standard CoT"},{id:"react",name:"ReAct (Reason+Act Framework)",year:2022,category:"General Reasoning",shortDescription:"Interleaving reasoning with tool use/actions. Produces reasoning traces alongside actions in an interwoven loop.",strengths:["35.1% exact-match on HotpotQA (vs 29% CoT)","10-34% improvement on interactive tasks","Human-like, interpretable chains"],weaknesses:["Requires external APIs or simulators","More complex prompting","Dependent on tool reliability"],referenceUrl:"https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/",benchmarkResults:"ReAct + CoT achieved 35.1% on HotpotQA vs ~29% with CoT alone"},{id:"pal",name:"Program-Aided Reasoning (PAL)",year:2022,category:"General Reasoning",shortDescription:"LLM generates code to solve problems. Outputs a Python program as the reasoning chain, executes it, returns the result.",strengths:["58.4% accuracy on GSM8K (vs 48.2% with CoT)","Eliminates final-step calculation errors","Verifiable reasoning via executed programs"],weaknesses:["Requires execution environment","Overhead of running code","Limited to problems expressible as code"],referenceUrl:"https://arxiv.org/abs/2211.10435",benchmarkResults:"PAL achieved 58.4% on GSM8K, surpassing 48.2% with plain CoT"},{id:"self-reflection",name:"Self-Reflection & Iterative Refinement",year:2023,category:"General Reasoning",shortDescription:"Models critique and refine their own outputs. Generates initial answer, inspects for errors, then improves upon it.",strengths:["88% pass@1 on HumanEval (vs 67% base GPT-4)","+12% accuracy on MBPP coding tasks","+9% on hardest Spider SQL queries"],weaknesses:["Increased computation from multiple generations","May over-correct or introduce new errors","Diminishing returns after few iterations"],referenceUrl:"https://arxiv.org/abs/2303.11366",benchmarkResults:"Reflexion-augmented GPT-4 achieved 88% pass@1 on HumanEval"},{id:"alphacode",name:"Massive Sample & Filter (AlphaCode)",year:2022,category:"Code Generation & Reasoning",shortDescription:"Generate many solutions and test them. Generates tens of thousands of solutions and filters by running against unit tests.",strengths:["Top 54.3% rank among human competitors","First AI to achieve median human performance","Robust against edge cases via filtering"],weaknesses:["Heavy computation (many samples)","Requires reliable test cases","Not practical for real-time applications"],referenceUrl:"https://deepmind.google/discover/blog/competitive-programming-with-alphacode/",benchmarkResults:"Achieved average rank in top 54.3% of Codeforces competitors"},{id:"self-debugging",name:"Self-Debugging & Automated Repair",year:2023,category:"Code Generation & Reasoning",shortDescription:"LLM fixes its own code errors. Runs the code, reads error messages, generates explanation of what went wrong, then corrects.",strengths:["+12% solve rate on MBPP","+2-3% on Spider SQL benchmark","+9% on hardest SQL queries"],weaknesses:["Requires execution environment","May enter infinite repair loops","Limited by model's debugging skills"],referenceUrl:"https://arxiv.org/abs/2304.05128",benchmarkResults:"Improved MBPP solve rate by 12 percentage points"},{id:"reflexion-code",name:"Reflexion / Iterative Prompting for Code",year:2023,category:"Code Generation & Reasoning",shortDescription:"Multiple attempts guided by self-reflection. Model tries, checks result, thinks aloud about errors, then attempts again.",strengths:["88% pass@1 on HumanEval","Near perfection on certain coding challenges","Eliminates trivial errors through iteration"],weaknesses:["Longer runtime (multiple model calls)","Careful prompt design required","May not overcome fundamental capability gaps"],referenceUrl:"https://arxiv.org/abs/2303.11366",benchmarkResults:"Reflexion-based agent reached 88% pass@1 on HumanEval"},{id:"code-cot",name:"Code Chain-of-Thought & Planning",year:2022,category:"Code Generation & Reasoning",shortDescription:"Reasoning steps in natural language before coding. Outlines solution approach or writes pseudo-code, then produces final code.",strengths:["Produces well-organized code","Reduces logical mistakes","Human-like planning for coding"],weaknesses:["Planning may not match execution needs","Adds overhead for simple tasks","Often combined with other methods"],referenceUrl:"https://arxiv.org/abs/2211.01910",benchmarkResults:"Improves success on multi-step coding puzzles when combined with Least-to-Most"},{id:"minerva",name:"CoT with Specialized Training (Minerva)",year:2022,category:"Mathematical Reasoning",shortDescription:"Training LLMs on step-by-step solutions. Fine-tuned on 118GB of math/science texts with CoT prompting + voting at inference.",strengths:["50.3% on MATH (vs previous 6.9%)","SOTA on GSM8K and STEM exams","Unlocks emergent math abilities"],weaknesses:["Needs large curated training data","Very large model sizes required","May not generalize to novel problem types"],referenceUrl:"https://research.google/blog/minerva-solving-quantitative-reasoning-problems-with-language-models/",benchmarkResults:"50.3% accuracy on MATH dataset vs previous best of 6.9%"},{id:"math-self-consistency",name:"Self-Consistency & Voting in Math",year:2022,category:"Mathematical Reasoning",shortDescription:"Mitigating calculation errors by consensus. Samples many CoT solutions and takes majority vote on final answer.",strengths:["+17.9% absolute accuracy on GSM8K","74% to 78.5% on GSM8K with voting","Boosts reliability without additional training"],weaknesses:["More inference passes required","Wrong answers may still converge","Doesn't improve underlying capabilities"],referenceUrl:"https://arxiv.org/abs/2203.11171",benchmarkResults:"Voting bumped PaLM 540B model's GSM8K score from ~74% to 78.5%"},{id:"pal-math",name:"Program-Aided Math Solving (PAL)",year:2022,category:"Mathematical Reasoning",shortDescription:"Language model writes a program to do the math. Outputs solvable program instead of numeric answer, defers to executor.",strengths:["48.2% to 58.4% on GSM8K","Introduces symbolic precision","Eliminates arithmetic errors"],weaknesses:["Requires runtime integration","May struggle if coding is incorrect","Problem must be code-expressible"],referenceUrl:"https://arxiv.org/abs/2211.10435",benchmarkResults:"Improved accuracy from 48.2% to 58.4% on GSM8K"},{id:"tot-puzzle",name:"Tree-of-Thought for Puzzle Solving",year:2023,category:"Mathematical Reasoning",shortDescription:"Systematic search for math puzzles. Explores different solution paths and backtracks when a path is invalid.",strengths:["74% on Game of 24 (vs <10% linear)","Dramatically boosts combinatorial problems","Ensures model doesn't get stuck"],weaknesses:["Complex control logic needed","More calls to the model","May not scale to very large search spaces"],referenceUrl:"https://arxiv.org/abs/2305.10601",benchmarkResults:"GPT-4 with ToT solved 74% vs <10% with linear reasoning"},{id:"least-to-most",name:"Least-to-Most Decomposition",year:2022,category:"Mathematical Reasoning",shortDescription:"Break complex problems into subproblems. Generates simpler sub-questions and solves them incrementally.",strengths:["Outperforms standard CoT on certain tasks","Reduces cognitive load per step","Mimics human problem-solving approach"],weaknesses:["Relies on correct subproblem generation","May miss holistic solution strategies","Can be tricky to decompose correctly"],referenceUrl:"https://arxiv.org/abs/2205.10625",benchmarkResults:"Higher accuracy than CoT on last letter concatenation puzzle"},{id:"cot-commonsense",name:"CoT + Self-Consistency (Commonsense)",year:2022,category:"Commonsense & Logical Reasoning",shortDescription:"Step-by-step reasoning for commonsense QA. Combines CoT with self-consistency for robust answers.",strengths:["+6.4% on StrategyQA","+3.9% on ARC-Challenge","Broadly applicable to logic puzzles"],weaknesses:["Smaller gains than in math","May not overcome knowledge gaps","Commonsense failures can be systematic"],referenceUrl:"https://arxiv.org/abs/2203.11171",benchmarkResults:"+6.4% on StrategyQA and +3.9% on ARC-Challenge"},{id:"decomposition",name:"Decomposition Prompting (Least-to-Most)",year:2022,category:"Commonsense & Logical Reasoning",shortDescription:"Explicitly break down complex questions. Generates intermediate questions and answers them sequentially.",strengths:["Higher accuracy than vanilla CoT","Handles multi-hop reasoning","Ensures each piece is handled"],weaknesses:["Decomposition errors propagate","Chain of subquestions can go astray","May lose context across sub-questions"],referenceUrl:"https://arxiv.org/abs/2205.10625",benchmarkResults:"Outperformed vanilla CoT on multi-step reasoning tasks"},{id:"ircot",name:"Iterative Retrieval + CoT (IRCoT)",year:2023,category:"Commonsense & Logical Reasoning",shortDescription:"Fetch relevant knowledge at each reasoning step. Interleaves retrieval with chain-of-thought reasoning.",strengths:["+15 points QA accuracy on HotpotQA","+21 points retrieval precision","Stays grounded in facts for multi-hop QA"],weaknesses:["Requires search API","Complex pipeline management","Retrieval latency adds up"],referenceUrl:"https://arxiv.org/abs/2212.10509",benchmarkResults:"Up to +15 points on QA accuracy and +21 on retrieval precision"},{id:"lot",name:"Logic-Augmented Prompting (LoT)",year:2024,category:"Commonsense & Logical Reasoning",shortDescription:"Inject formal logic structure into reasoning. Extracts propositional logic from text as additional context.",strengths:["+4.35% on ReClor (LSAT-like QA)","+5% on LogiQA with CoT+SC","+8% on ProofWriter with ToT"],weaknesses:["Needs reliable text-to-logic parsing","May not generalize to all problems","Added complexity in prompt design"],referenceUrl:"https://arxiv.org/abs/2401.04073",benchmarkResults:"+4.35% on ReClor, +5% on LogiQA, +8% on ProofWriter"},{id:"neuro-symbolic",name:"Hybrid Neuro-Symbolic Solvers",year:2023,category:"Commonsense & Logical Reasoning",shortDescription:"External logical tools with LLM guidance. LLMs call dedicated logic engines (e.g., SAT solvers).",strengths:["Perfect accuracy on certain puzzle tasks","Combines language understanding with symbolic rigor","Guaranteed logical reasoning"],weaknesses:["Complex system integration","Requires domain-specific solvers","Less common in general usage"],referenceUrl:"https://arxiv.org/abs/2304.09102",benchmarkResults:"Can achieve perfect accuracy on logic grid puzzles"},{id:"multimodal-cot",name:"Multimodal Chain-of-Thought",year:2022,category:"Multi-Modal Reasoning",shortDescription:"Reasoning chains that include visual inputs. Produces explanations that reference image content before answering.",strengths:["90.45% on ScienceQA (vs 86.54% prior)","~4% jump on visual reasoning tasks","Transparent decision process"],weaknesses:["Requires multimodal training","Model must understand text and images jointly","Visual reasoning steps can be imprecise"],referenceUrl:"https://arxiv.org/abs/2302.00923",benchmarkResults:"90.45% accuracy on ScienceQA, improving over prior SOTA of 86.54%"},{id:"palm-e",name:"Embodied Multimodal Models (PaLM-E)",year:2023,category:"Multi-Modal Reasoning",shortDescription:"Feeding visual features into giant language models. Connects 540B LM with vision encoders for multimodal reasoning.",strengths:["66.1% on OK-VQA (outperforms specialists)","Excels in captioning and planning","Seamless text and vision reasoning"],weaknesses:["Sheer model size and complexity","Expensive training requirements","Limited to specific embodied domains"],referenceUrl:"https://palm-e.github.io/",benchmarkResults:"66.1% on OK-VQA, outperforming specialist models"},{id:"tool-visual",name:"Tool-Augmented Visual Reasoning",year:2023,category:"Multi-Modal Reasoning",shortDescription:"Using external tools on visual tasks via reasoning. Allows VLMs to call OCR, calculators, or vision APIs.",strengths:["Addresses pure vision-to-text failures","Significantly improves chart QA accuracy","Modular and extensible"],weaknesses:["Pipeline complexity","Errors can compound across tools","Tool integration overhead"],referenceUrl:"https://arxiv.org/abs/2303.04671",benchmarkResults:"Solves problems no single model could solve alone on diagram reasoning"},{id:"vlm-cot",name:"Large VLMs with CoT (GPT-4V & others)",year:2023,category:"Multi-Modal Reasoning",shortDescription:"Very large multimodal models that inherently reason. Reasons through image content stepwise to answer queries.",strengths:["Exceeds human performance on VQAv2 metrics","Benefits from all text-domain advances","Unified architecture simplicity"],weaknesses:["Huge scale required","Difficulty evaluating reasoning vs pattern matching","May hallucinate visual details"],referenceUrl:"https://openai.com/research/gpt-4v-system-card",benchmarkResults:"GPT-4V exceeds human performance on some VQA metrics"}];var A=a.i(64756),B=a.i(19759),C=a.i(88959),D=a.i(46774);let E=[{id:"elite",title:"Elite Mode",description:"Industry-leading orchestration strategies",icon:v,color:"from-yellow-500 to-amber-600",isPremium:!0},{id:"models",title:"Models",description:"Select AI models for multi-agent orchestration",icon:k.Cpu,color:"from-orange-500 to-amber-500"},{id:"reasoning",title:"Reasoning",description:"Advanced reasoning methods and techniques",icon:l.Brain,color:"from-purple-500 to-indigo-500"},{id:"tuning",title:"Tuning",description:"Fine-tune response behavior and validation",icon:m.Sliders,color:"from-emerald-500 to-teal-500"},{id:"features",title:"Features",description:"RAG, MCP, memory and more capabilities",icon:n.Wrench,color:"from-blue-500 to-cyan-500"},{id:"tools",title:"Tools",description:"External integrations and tool access",icon:p,color:"from-rose-500 to-pink-500"},{id:"quality",title:"Quality",description:"Verification, challenge loops and fact-checking",icon:w.Shield,color:"from-green-500 to-emerald-600"},{id:"speed",title:"Speed",description:"Response speed and processing depth",icon:u.Zap,color:"from-cyan-500 to-sky-500"}],F=[{id:"fast",label:"Fast",description:"Quick single-model responses",confidence:"70%"},{id:"standard",label:"Standard",description:"Multi-model with verification",confidence:"80%"},{id:"thorough",label:"Thorough",description:"Full pipeline with challenge loop",confidence:"90%"},{id:"exhaustive",label:"Exhaustive",description:"All techniques including debate",confidence:"95%"}],G=[{id:"verification",label:"Fact Verification",description:"Verify all factual claims"},{id:"challenge",label:"Challenge Loop",description:"Adversarial stress-testing of answers"},{id:"consensus",label:"Multi-Model Consensus",description:"Agreement between multiple models"},{id:"reflection",label:"Self-Reflection",description:"Models critique and improve their output"},{id:"tools",label:"Tool Integration",description:"Use search, calculator, code execution"}],H=[{id:"vector-rag",label:"Vector DB + RAG",description:"Retrieval augmented generation with vector database"},{id:"mcp-server",label:"MCP Server + Tools",description:"Model context protocol for tool orchestration"},{id:"personal-database",label:"Personal Database",description:"Your private knowledge base"},{id:"modular-answer-feed",label:"Modular Answer Feed",description:"Internal LLM routing and composition"},{id:"memory-augmentation",label:"Memory Augmentation",description:"Long-term conversation memory"},{id:"code-interpreter",label:"Code Interpreter",description:"Execute code in a sandboxed environment"}],I=[{id:"web-search",label:"Web Search",description:"Search the internet for real-time information"},{id:"code-execution",label:"Code Execution",description:"Run Python, JavaScript, and more"},{id:"file-analysis",label:"File Analysis",description:"Parse and analyze documents"},{id:"image-generation",label:"Image Generation",description:"Create images from text"},{id:"data-visualization",label:"Data Visualization",description:"Generate charts and graphs"},{id:"api-integration",label:"API Integration",description:"Connect to external APIs"}],J=[{key:"promptOptimization",label:"Prompt Optimization",description:"Automatically enhance prompts",icon:t.Sparkles},{key:"outputValidation",label:"Output Validation",description:"Verify and fact-check responses",icon:r.Check},{key:"answerStructure",label:"Answer Structure",description:"Format with clear sections",icon:q.Layers},{key:"sharedMemory",label:"Shared Memory",description:"Access previous conversations",icon:p},{key:"learnFromChat",label:"Learn from Chat",description:"Improve from this conversation",icon:n.Wrench}],K=[{id:"temperature",label:"Temperature",description:"Controls randomness (0-2)",min:0,max:2,step:.1,default:.7},{id:"maxTokens",label:"Max Tokens",description:"Maximum response length",min:100,max:4e3,step:100,default:2e3},{id:"topP",label:"Top P",description:"Nucleus sampling threshold",min:0,max:1,step:.05,default:.9},{id:"frequencyPenalty",label:"Frequency Penalty",description:"Reduce repetition",min:0,max:2,step:.1,default:0},{id:"presencePenalty",label:"Presence Penalty",description:"Encourage new topics",min:0,max:2,step:.1,default:0}],L=[{id:"fast",label:"Fast",description:"Quick responses with minimal processing"},{id:"standard",label:"Standard",description:"Balanced speed and quality"},{id:"deep",label:"Deep",description:"Thorough analysis with extended processing"}];function M(){let a=(0,d.useRouter)(),[o,t]=(0,c.useState)(null),[M,N]=(0,c.useState)(!1),[O,P]=(0,c.useState)(null),[Q,R]=(0,c.useState)(!1),[S,T]=(0,c.useState)(["automatic"]),[U,V]=(0,c.useState)([]),[W,X]=(0,c.useState)([]),[Y,Z]=(0,c.useState)([]),[$,_]=(0,c.useState)({promptOptimization:!0,outputValidation:!0,answerStructure:!1,sharedMemory:!0,learnFromChat:!1}),[aa,ab]=(0,c.useState)({temperature:.7,maxTokens:2e3,topP:.9,frequencyPenalty:0,presencePenalty:0}),[ac,ad]=(0,c.useState)("standard"),[ae,af]=(0,c.useState)("standard"),[ag,ah]=(0,c.useState)(["verification","consensus"]);return(0,c.useEffect)(()=>{let a=(0,C.loadOrchestratorSettings)();T(a.selectedModels||["automatic"]),V(a.advancedReasoningMethods||[]),X(a.advancedFeatures||[]),Z(a.advancedFeatures?.filter(a=>["web-search","code-execution","file-analysis","image-generation","data-visualization","api-integration"].includes(a))||[]),_({promptOptimization:a.promptOptimization,outputValidation:a.outputValidation,answerStructure:a.answerStructure,sharedMemory:a.sharedMemory,learnFromChat:a.learnFromChat}),ad(a.reasoningMode||"standard"),af(a.eliteStrategy||"standard"),ah(a.qualityOptions||["verification","consensus"]),a.standardValues&&ab(a.standardValues),R(!0)},[]),(0,c.useEffect)(()=>{Q&&(0,C.saveOrchestratorSettings)({selectedModels:S,advancedReasoningMethods:U,advancedFeatures:[...W,...Y],promptOptimization:$.promptOptimization,outputValidation:$.outputValidation,answerStructure:$.answerStructure,sharedMemory:$.sharedMemory,learnFromChat:$.learnFromChat,reasoningMode:ac,eliteStrategy:ae,qualityOptions:ag,standardValues:aa})},[S,U,W,Y,$,ac,ae,ag,aa,Q]),(0,b.jsxs)("div",{className:"flex h-screen bg-background",children:[(0,b.jsx)(A.Sidebar,{collapsed:M,onToggleCollapse:()=>N(!M),conversations:[],currentConversationId:null,onSelectConversation:()=>a.push(D.ROUTES.HOME),onNewChat:()=>a.push(D.ROUTES.HOME),onDeleteConversation:()=>{},onTogglePin:()=>{},onRenameConversation:()=>{},onMoveToProject:()=>{},projects:[],onGoHome:()=>a.push(D.ROUTES.HOME)}),(0,b.jsxs)("div",{className:"flex-1 flex flex-col min-h-0 overflow-hidden",children:[(0,b.jsx)("div",{className:"hidden md:flex items-center justify-end p-3 border-b border-border bg-card/50",children:(0,b.jsx)(B.UserAccountMenu,{user:O,onSignIn:()=>{},onSignOut:()=>P(null)})}),(0,b.jsx)("div",{className:"flex-1 h-full overflow-auto",children:(0,b.jsxs)("div",{className:"min-h-full flex flex-col items-center justify-start px-4 pt-0 pb-20",children:[(0,b.jsxs)("div",{className:"text-center mb-0",children:[(0,b.jsx)("div",{className:"relative w-40 h-40 md:w-[280px] md:h-[280px] lg:w-[320px] lg:h-[320px] mx-auto mb-0 -mt-4 md:-mt-8 lg:-mt-10",children:(0,b.jsx)(e.default,{src:"/logo.png",alt:"LLMHive",fill:!0,className:"object-contain",priority:!0})}),(0,b.jsx)("h1",{className:"-mt-6 md:-mt-8 lg:-mt-10 text-[1.75rem] md:text-[2.85rem] lg:text-[3.4rem] font-bold mb-1 bg-gradient-to-r from-[var(--bronze)] via-[var(--gold)] to-[var(--bronze)] bg-clip-text text-transparent",children:"Orchestration"}),(0,b.jsx)("p",{className:"text-muted-foreground text-sm md:text-base max-w-md mx-auto mb-0",children:"Configure your multi-agent AI system with precision"})]}),(0,b.jsx)("div",{className:"w-16 h-px bg-border my-2"}),(0,b.jsxs)("div",{className:"w-full max-w-5xl",children:[(0,b.jsx)("p",{className:"text-sm text-muted-foreground text-center mb-2",children:"Select a category to configure"}),(0,b.jsx)("div",{className:"grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 xl:grid-cols-6 gap-3",children:E.map(a=>{let c=a.icon,d=(a=>{switch(a){case"models":return S.length;case"reasoning":return U.length;case"features":return W.length;case"tools":return Y.length;case"tuning":return Object.values($).filter(Boolean).length;case"speed":case"elite":return 1;case"quality":return ag.length;default:return 0}})(a.id);return(0,b.jsxs)("button",{onClick:()=>t(a.id),className:"group flex flex-col items-center gap-2 p-3 md:p-4 rounded-xl border border-border hover:border-[var(--bronze)] bg-card/50 hover:bg-card/80 transition-all duration-300 cursor-pointer text-left relative",children:[d>0&&(0,b.jsx)(h.Badge,{className:"absolute top-2 right-2 bg-[var(--bronze)] text-black text-[10px] px-1.5 py-0.5 min-w-[18px] h-[18px] flex items-center justify-center",children:d}),(0,b.jsx)("div",{className:`w-10 h-10 md:w-12 md:h-12 rounded-xl bg-gradient-to-br ${a.color} flex items-center justify-center shadow-lg group-hover:scale-110 group-hover:shadow-xl transition-all duration-300`,children:(0,b.jsx)(c,{className:"h-5 w-5 md:h-6 md:w-6 text-white"})}),(0,b.jsxs)("div",{className:"text-center",children:[(0,b.jsx)("h3",{className:"text-sm md:text-base font-semibold text-foreground group-hover:text-[var(--bronze)] transition-colors",children:a.title}),(0,b.jsx)("p",{className:"text-xs text-muted-foreground mt-1 line-clamp-2",children:a.description})]})]},a.id)})})]})]})})]}),(0,b.jsx)(j.Sheet,{open:"models"===o,onOpenChange:a=>!a&&t(null),children:(0,b.jsxs)(j.SheetContent,{side:"right",className:"w-[260px] sm:w-[280px] bg-background/95 backdrop-blur-xl border-l border-border/50 p-0",children:[(0,b.jsx)("div",{className:"p-5 border-b border-border/50",children:(0,b.jsxs)(j.SheetHeader,{className:"space-y-1",children:[(0,b.jsxs)(j.SheetTitle,{className:"text-base font-medium flex items-center gap-2",children:[(0,b.jsx)("div",{className:"w-8 h-8 rounded-lg bg-gradient-to-br from-blue-500/20 to-cyan-500/20 flex items-center justify-center",children:(0,b.jsx)(k.Cpu,{className:"h-4 w-4 text-blue-400"})}),"Models",(0,b.jsxs)("span",{className:"ml-auto text-xs font-normal text-[var(--bronze)] bg-[var(--bronze)]/10 px-2 py-0.5 rounded-full",children:[S.length," selected"]})]}),(0,b.jsx)("p",{className:"text-xs text-muted-foreground",children:"Select AI models for orchestration"})]})}),(0,b.jsx)(i.ScrollArea,{className:"h-[calc(100vh-100px)]",children:(0,b.jsx)("div",{className:"p-4 space-y-1.5",children:x.AVAILABLE_MODELS.map(a=>{let c=S.includes(a.id);return(0,b.jsx)("div",{onClick:()=>{var b;return b=a.id,void T(a=>a.includes(b)?a.filter(a=>a!==b):[...a,b])},className:`group p-2.5 rounded-lg cursor-pointer transition-all duration-200 ${c?"bg-[var(--bronze)]/10 ring-1 ring-[var(--bronze)]/30":"hover:bg-muted/50"}`,children:(0,b.jsxs)("div",{className:"flex items-center gap-2.5",children:[(0,b.jsx)("div",{className:`w-4 h-4 rounded border-2 flex items-center justify-center transition-all flex-shrink-0 ${c?"border-[var(--bronze)] bg-[var(--bronze)]":"border-muted-foreground/30 group-hover:border-muted-foreground/50"}`,children:c&&(0,b.jsx)(r.Check,{className:"h-2.5 w-2.5 text-background"})}),(0,b.jsx)("div",{className:"w-6 h-6 relative flex-shrink-0 rounded-md overflow-hidden bg-muted/50",children:(0,b.jsx)(e.default,{src:(0,x.getModelLogo)(a.provider)||"/placeholder.svg",alt:a.provider,fill:!0,className:"object-contain p-0.5"})}),(0,b.jsxs)("div",{className:"flex-1 min-w-0",children:[(0,b.jsx)("p",{className:`text-sm font-medium truncate ${c?"text-[var(--bronze)]":""}`,children:a.name}),(0,b.jsx)("p",{className:"text-[10px] text-muted-foreground capitalize",children:a.provider})]})]})},a.id)})})})]})}),(0,b.jsx)(j.Sheet,{open:"reasoning"===o,onOpenChange:a=>!a&&t(null),children:(0,b.jsxs)(j.SheetContent,{side:"right",className:"w-[260px] sm:w-[280px] bg-background/95 backdrop-blur-xl border-l border-border/50 p-0",children:[(0,b.jsx)("div",{className:"p-5 border-b border-border/50",children:(0,b.jsxs)(j.SheetHeader,{className:"space-y-1",children:[(0,b.jsxs)(j.SheetTitle,{className:"text-base font-medium flex items-center gap-2",children:[(0,b.jsx)("div",{className:"w-8 h-8 rounded-lg bg-gradient-to-br from-purple-500/20 to-pink-500/20 flex items-center justify-center",children:(0,b.jsx)(l.Brain,{className:"h-4 w-4 text-purple-400"})}),"Reasoning",(0,b.jsxs)("span",{className:"ml-auto text-xs font-normal text-[var(--bronze)] bg-[var(--bronze)]/10 px-2 py-0.5 rounded-full",children:[U.length," selected"]})]}),(0,b.jsx)("p",{className:"text-xs text-muted-foreground",children:"Advanced reasoning methods"})]})}),(0,b.jsx)(i.ScrollArea,{className:"h-[calc(100vh-100px)]",children:(0,b.jsx)("div",{className:"p-4",children:y.map(a=>{let c=z.filter(b=>b.category===a);return(0,b.jsxs)("div",{className:"mb-5",children:[(0,b.jsx)("h3",{className:"text-[10px] font-semibold uppercase tracking-wider text-muted-foreground mb-2 px-1",children:a}),(0,b.jsx)("div",{className:"space-y-1.5",children:c.map(a=>{let c=U.includes(a.id);return(0,b.jsx)("div",{onClick:()=>{var b;return b=a.id,void V(a=>a.includes(b)?a.filter(a=>a!==b):[...a,b])},className:`group p-2.5 rounded-lg cursor-pointer transition-all duration-200 ${c?"bg-[var(--bronze)]/10 ring-1 ring-[var(--bronze)]/30":"hover:bg-muted/50"}`,children:(0,b.jsxs)("div",{className:"flex items-center gap-2.5",children:[(0,b.jsx)("div",{className:`w-4 h-4 rounded border-2 flex items-center justify-center transition-all flex-shrink-0 ${c?"border-[var(--bronze)] bg-[var(--bronze)]":"border-muted-foreground/30 group-hover:border-muted-foreground/50"}`,children:c&&(0,b.jsx)(r.Check,{className:"h-2.5 w-2.5 text-background"})}),(0,b.jsx)("div",{className:"flex-1 min-w-0",children:(0,b.jsxs)("div",{className:"flex items-center gap-1.5",children:[(0,b.jsx)("p",{className:"text-sm font-medium truncate",children:a.name}),(0,b.jsx)("span",{className:"text-[9px] text-muted-foreground bg-muted/50 px-1 rounded",children:a.year})]})}),(0,b.jsx)("a",{href:a.referenceUrl,target:"_blank",rel:"noopener noreferrer",className:"opacity-0 group-hover:opacity-100 transition-opacity",onClick:a=>a.stopPropagation(),children:(0,b.jsx)(s.ExternalLink,{className:"h-3 w-3 text-muted-foreground hover:text-[var(--bronze)]"})})]})},a.id)})})]},a)})})})]})}),(0,b.jsx)(j.Sheet,{open:"tuning"===o,onOpenChange:a=>!a&&t(null),children:(0,b.jsxs)(j.SheetContent,{side:"right",className:"w-[260px] sm:w-[280px] bg-background/95 backdrop-blur-xl border-l border-border/50 p-0",children:[(0,b.jsx)("div",{className:"p-5 border-b border-border/50",children:(0,b.jsxs)(j.SheetHeader,{className:"space-y-1",children:[(0,b.jsxs)(j.SheetTitle,{className:"text-base font-medium flex items-center gap-2",children:[(0,b.jsx)("div",{className:"w-8 h-8 rounded-lg bg-gradient-to-br from-amber-500/20 to-orange-500/20 flex items-center justify-center",children:(0,b.jsx)(m.Sliders,{className:"h-4 w-4 text-amber-400"})}),"Tuning",(0,b.jsxs)("span",{className:"ml-auto text-xs font-normal text-[var(--bronze)] bg-[var(--bronze)]/10 px-2 py-0.5 rounded-full",children:[Object.values($).filter(Boolean).length," enabled"]})]}),(0,b.jsx)("p",{className:"text-xs text-muted-foreground",children:"Fine-tune response behavior"})]})}),(0,b.jsx)(i.ScrollArea,{className:"h-[calc(100vh-100px)]",children:(0,b.jsx)("div",{className:"p-4 space-y-1.5",children:J.map(a=>{let c=$[a.key];return(0,b.jsx)("div",{onClick:()=>_(b=>({...b,[a.key]:!b[a.key]})),className:`group p-2.5 rounded-lg cursor-pointer transition-all duration-200 ${c?"bg-[var(--bronze)]/10 ring-1 ring-[var(--bronze)]/30":"hover:bg-muted/50"}`,children:(0,b.jsxs)("div",{className:"flex items-center gap-2.5",children:[(0,b.jsx)("div",{className:`w-4 h-4 rounded border-2 flex items-center justify-center transition-all flex-shrink-0 ${c?"border-[var(--bronze)] bg-[var(--bronze)]":"border-muted-foreground/30 group-hover:border-muted-foreground/50"}`,children:c&&(0,b.jsx)(r.Check,{className:"h-2.5 w-2.5 text-background"})}),(0,b.jsxs)("div",{className:"flex-1 min-w-0",children:[(0,b.jsx)("p",{className:`text-sm font-medium ${c?"text-[var(--bronze)]":""}`,children:a.label}),(0,b.jsx)("p",{className:"text-[10px] text-muted-foreground leading-tight",children:a.description})]})]})},a.key)})})})]})}),(0,b.jsx)(j.Sheet,{open:"features"===o,onOpenChange:a=>!a&&t(null),children:(0,b.jsxs)(j.SheetContent,{side:"right",className:"w-[260px] sm:w-[280px] bg-background/95 backdrop-blur-xl border-l border-border/50 p-0",children:[(0,b.jsx)("div",{className:"p-5 border-b border-border/50",children:(0,b.jsxs)(j.SheetHeader,{className:"space-y-1",children:[(0,b.jsxs)(j.SheetTitle,{className:"text-base font-medium flex items-center gap-2",children:[(0,b.jsx)("div",{className:"w-8 h-8 rounded-lg bg-gradient-to-br from-green-500/20 to-emerald-500/20 flex items-center justify-center",children:(0,b.jsx)(n.Wrench,{className:"h-4 w-4 text-green-400"})}),"Features",(0,b.jsxs)("span",{className:"ml-auto text-xs font-normal text-[var(--bronze)] bg-[var(--bronze)]/10 px-2 py-0.5 rounded-full",children:[W.length," enabled"]})]}),(0,b.jsx)("p",{className:"text-xs text-muted-foreground",children:"Advanced capabilities"})]})}),(0,b.jsx)(i.ScrollArea,{className:"h-[calc(100vh-100px)]",children:(0,b.jsx)("div",{className:"p-4 space-y-1.5",children:H.map(a=>{let c=W.includes(a.id);return(0,b.jsx)("div",{onClick:()=>{var b;return b=a.id,void X(a=>a.includes(b)?a.filter(a=>a!==b):[...a,b])},className:`group p-2.5 rounded-lg cursor-pointer transition-all duration-200 ${c?"bg-[var(--bronze)]/10 ring-1 ring-[var(--bronze)]/30":"hover:bg-muted/50"}`,children:(0,b.jsxs)("div",{className:"flex items-center gap-2.5",children:[(0,b.jsx)("div",{className:`w-4 h-4 rounded border-2 flex items-center justify-center transition-all flex-shrink-0 ${c?"border-[var(--bronze)] bg-[var(--bronze)]":"border-muted-foreground/30 group-hover:border-muted-foreground/50"}`,children:c&&(0,b.jsx)(r.Check,{className:"h-2.5 w-2.5 text-background"})}),(0,b.jsxs)("div",{className:"flex-1 min-w-0",children:[(0,b.jsx)("p",{className:`text-sm font-medium ${c?"text-[var(--bronze)]":""}`,children:a.label}),(0,b.jsx)("p",{className:"text-[10px] text-muted-foreground leading-tight",children:a.description})]})]})},a.id)})})})]})}),(0,b.jsx)(j.Sheet,{open:"tools"===o,onOpenChange:a=>!a&&t(null),children:(0,b.jsxs)(j.SheetContent,{side:"right",className:"w-[260px] sm:w-[280px] bg-background/95 backdrop-blur-xl border-l border-border/50 p-0",children:[(0,b.jsx)("div",{className:"p-5 border-b border-border/50",children:(0,b.jsxs)(j.SheetHeader,{className:"space-y-1",children:[(0,b.jsxs)(j.SheetTitle,{className:"text-base font-medium flex items-center gap-2",children:[(0,b.jsx)("div",{className:"w-8 h-8 rounded-lg bg-gradient-to-br from-rose-500/20 to-red-500/20 flex items-center justify-center",children:(0,b.jsx)(p,{className:"h-4 w-4 text-rose-400"})}),"Tools",(0,b.jsxs)("span",{className:"ml-auto text-xs font-normal text-[var(--bronze)] bg-[var(--bronze)]/10 px-2 py-0.5 rounded-full",children:[Y.length," enabled"]})]}),(0,b.jsx)("p",{className:"text-xs text-muted-foreground",children:"External integrations"})]})}),(0,b.jsx)(i.ScrollArea,{className:"h-[calc(100vh-100px)]",children:(0,b.jsx)("div",{className:"p-4 space-y-1.5",children:I.map(a=>{let c=Y.includes(a.id);return(0,b.jsx)("div",{onClick:()=>{var b;return b=a.id,void Z(a=>a.includes(b)?a.filter(a=>a!==b):[...a,b])},className:`group p-2.5 rounded-lg cursor-pointer transition-all duration-200 ${c?"bg-[var(--bronze)]/10 ring-1 ring-[var(--bronze)]/30":"hover:bg-muted/50"}`,children:(0,b.jsxs)("div",{className:"flex items-center gap-2.5",children:[(0,b.jsx)("div",{className:`w-4 h-4 rounded border-2 flex items-center justify-center transition-all flex-shrink-0 ${c?"border-[var(--bronze)] bg-[var(--bronze)]":"border-muted-foreground/30 group-hover:border-muted-foreground/50"}`,children:c&&(0,b.jsx)(r.Check,{className:"h-2.5 w-2.5 text-background"})}),(0,b.jsxs)("div",{className:"flex-1 min-w-0",children:[(0,b.jsx)("p",{className:`text-sm font-medium ${c?"text-[var(--bronze)]":""}`,children:a.label}),(0,b.jsx)("p",{className:"text-[10px] text-muted-foreground leading-tight",children:a.description})]})]})},a.id)})})})]})}),(0,b.jsx)(j.Sheet,{open:"standard"===o,onOpenChange:a=>!a&&t(null),children:(0,b.jsxs)(j.SheetContent,{side:"right",className:"w-[260px] sm:w-[280px] bg-background/95 backdrop-blur-xl border-l border-border/50 p-0",children:[(0,b.jsx)("div",{className:"p-4 border-b border-border/50",children:(0,b.jsxs)(j.SheetHeader,{className:"space-y-1",children:[(0,b.jsxs)(j.SheetTitle,{className:"text-sm font-medium flex items-center gap-2",children:[(0,b.jsx)("div",{className:"w-6 h-6 rounded-md bg-gradient-to-br from-indigo-500/20 to-violet-500/20 flex items-center justify-center",children:(0,b.jsx)(q.Layers,{className:"h-3 w-3 text-indigo-400"})}),"Standard"]}),(0,b.jsx)("p",{className:"text-[10px] text-muted-foreground",children:"Temperature, tokens & sampling"})]})}),(0,b.jsx)("div",{className:"p-4 space-y-4",children:K.map(a=>(0,b.jsxs)("div",{className:"space-y-2",children:[(0,b.jsxs)("div",{className:"flex items-center justify-between",children:[(0,b.jsx)(f.Label,{className:"text-xs font-medium",children:a.label}),(0,b.jsx)("span",{className:"text-[10px] font-mono text-[var(--bronze)] bg-[var(--bronze)]/10 px-1.5 py-0.5 rounded",children:aa[a.id]})]}),(0,b.jsx)(g.Slider,{value:[aa[a.id]],onValueChange:([b])=>ab(c=>({...c,[a.id]:b})),min:a.min,max:a.max,step:a.step,className:"[&_[role=slider]]:bg-[var(--bronze)] [&_[role=slider]]:border-[var(--bronze)] [&_[role=slider]]:w-3 [&_[role=slider]]:h-3 [&_.bg-primary]:bg-[var(--bronze)]"}),(0,b.jsx)("p",{className:"text-[9px] text-muted-foreground",children:a.description})]},a.id))})]})}),(0,b.jsx)(j.Sheet,{open:"speed"===o,onOpenChange:a=>!a&&t(null),children:(0,b.jsxs)(j.SheetContent,{side:"right",className:"w-[260px] sm:w-[280px] bg-background/95 backdrop-blur-xl border-l border-border/50 p-0",children:[(0,b.jsx)("div",{className:"p-4 border-b border-border/50",children:(0,b.jsxs)(j.SheetHeader,{className:"space-y-1",children:[(0,b.jsxs)(j.SheetTitle,{className:"text-sm font-medium flex items-center gap-2",children:[(0,b.jsx)("div",{className:"w-6 h-6 rounded-md bg-gradient-to-br from-cyan-500/20 to-sky-500/20 flex items-center justify-center",children:(0,b.jsx)(u.Zap,{className:"h-3 w-3 text-cyan-400"})}),"Speed"]}),(0,b.jsx)("p",{className:"text-[10px] text-muted-foreground",children:"Response speed & processing depth"})]})}),(0,b.jsx)(i.ScrollArea,{className:"h-[calc(100vh-120px)]",children:(0,b.jsx)("div",{className:"p-3 space-y-1.5",children:L.map(a=>{let c=ac===a.id;return(0,b.jsxs)("button",{onClick:()=>ad(a.id),className:`w-full flex items-center gap-2.5 p-2.5 rounded-lg transition-all duration-200 ${c?"bg-[var(--bronze)]/10 ring-1 ring-[var(--bronze)]/30":"hover:bg-muted/50"}`,children:[(0,b.jsx)("div",{className:`w-4 h-4 rounded border-2 flex items-center justify-center transition-all ${c?"bg-[var(--bronze)] border-[var(--bronze)]":"border-muted-foreground/30"}`,children:c&&(0,b.jsx)(r.Check,{className:"h-2.5 w-2.5 text-black"})}),(0,b.jsxs)("div",{className:"flex-1 text-left",children:[(0,b.jsx)("p",{className:`text-xs font-medium ${c?"text-[var(--bronze)]":"text-foreground"}`,children:a.label}),(0,b.jsx)("p",{className:"text-[10px] text-muted-foreground",children:a.description})]})]},a.id)})})})]})}),(0,b.jsx)(j.Sheet,{open:"elite"===o,onOpenChange:a=>!a&&t(null),children:(0,b.jsxs)(j.SheetContent,{side:"right",className:"w-[260px] sm:w-[280px] bg-background/95 backdrop-blur-xl border-l border-border/50 p-0",children:[(0,b.jsx)("div",{className:"p-5 border-b border-border/50",children:(0,b.jsxs)(j.SheetHeader,{className:"space-y-1",children:[(0,b.jsxs)(j.SheetTitle,{className:"text-base font-medium flex items-center gap-2",children:[(0,b.jsx)("div",{className:"w-8 h-8 rounded-lg bg-gradient-to-br from-yellow-500/20 to-amber-500/20 flex items-center justify-center",children:(0,b.jsx)(v,{className:"h-4 w-4 text-yellow-400"})}),"Elite Mode",(0,b.jsx)(h.Badge,{className:"ml-auto bg-gradient-to-r from-yellow-500 to-amber-600 text-white text-[10px]",children:"PRO"})]}),(0,b.jsx)("p",{className:"text-xs text-muted-foreground",children:"Industry-leading orchestration strategies"})]})}),(0,b.jsx)(i.ScrollArea,{className:"h-[calc(100vh-120px)]",children:(0,b.jsxs)("div",{className:"p-4 space-y-3",children:[(0,b.jsx)("p",{className:"text-[10px] uppercase tracking-wider text-muted-foreground font-semibold",children:"Orchestration Strategy"}),F.map(a=>{let c=ae===a.id;return(0,b.jsxs)("button",{onClick:()=>af(a.id),className:`w-full p-3 rounded-lg transition-all duration-200 text-left ${c?"bg-[var(--bronze)]/10 ring-1 ring-[var(--bronze)]/30":"hover:bg-muted/50 border border-border"}`,children:[(0,b.jsxs)("div",{className:"flex items-center justify-between mb-1",children:[(0,b.jsxs)("div",{className:"flex items-center gap-2",children:[(0,b.jsx)("div",{className:`w-4 h-4 rounded-full border-2 flex items-center justify-center transition-all ${c?"bg-[var(--bronze)] border-[var(--bronze)]":"border-muted-foreground/30"}`,children:c&&(0,b.jsx)(r.Check,{className:"h-2.5 w-2.5 text-black"})}),(0,b.jsx)("span",{className:`text-sm font-medium ${c?"text-[var(--bronze)]":""}`,children:a.label})]}),(0,b.jsx)(h.Badge,{variant:"secondary",className:"text-[9px] bg-green-500/10 text-green-400",children:a.confidence})]}),(0,b.jsx)("p",{className:"text-[10px] text-muted-foreground ml-6",children:a.description})]},a.id)}),(0,b.jsxs)("div",{className:"pt-4 border-t border-border/50 mt-4",children:[(0,b.jsx)("p",{className:"text-[10px] uppercase tracking-wider text-muted-foreground font-semibold mb-3",children:"How It Works"}),(0,b.jsxs)("div",{className:"space-y-2 text-[10px] text-muted-foreground",children:[(0,b.jsxs)("p",{children:["• ",(0,b.jsx)("strong",{children:"Fast:"})," Single best model, quick responses"]}),(0,b.jsxs)("p",{children:["• ",(0,b.jsx)("strong",{children:"Standard:"})," Multi-model with quality fusion"]}),(0,b.jsxs)("p",{children:["• ",(0,b.jsx)("strong",{children:"Thorough:"})," Challenge loop stress-tests answers"]}),(0,b.jsxs)("p",{children:["• ",(0,b.jsx)("strong",{children:"Exhaustive:"})," Expert panel + debate + verification"]})]})]})]})})]})}),(0,b.jsx)(j.Sheet,{open:"quality"===o,onOpenChange:a=>!a&&t(null),children:(0,b.jsxs)(j.SheetContent,{side:"right",className:"w-[260px] sm:w-[280px] bg-background/95 backdrop-blur-xl border-l border-border/50 p-0",children:[(0,b.jsx)("div",{className:"p-5 border-b border-border/50",children:(0,b.jsxs)(j.SheetHeader,{className:"space-y-1",children:[(0,b.jsxs)(j.SheetTitle,{className:"text-base font-medium flex items-center gap-2",children:[(0,b.jsx)("div",{className:"w-8 h-8 rounded-lg bg-gradient-to-br from-green-500/20 to-emerald-500/20 flex items-center justify-center",children:(0,b.jsx)(w.Shield,{className:"h-4 w-4 text-green-400"})}),"Quality",(0,b.jsxs)("span",{className:"ml-auto text-xs font-normal text-[var(--bronze)] bg-[var(--bronze)]/10 px-2 py-0.5 rounded-full",children:[ag.length," enabled"]})]}),(0,b.jsx)("p",{className:"text-xs text-muted-foreground",children:"Verification and quality assurance"})]})}),(0,b.jsx)(i.ScrollArea,{className:"h-[calc(100vh-100px)]",children:(0,b.jsx)("div",{className:"p-4 space-y-1.5",children:G.map(a=>{let c=ag.includes(a.id);return(0,b.jsx)("div",{onClick:()=>{var b;return b=a.id,void ah(a=>a.includes(b)?a.filter(a=>a!==b):[...a,b])},className:`group p-2.5 rounded-lg cursor-pointer transition-all duration-200 ${c?"bg-[var(--bronze)]/10 ring-1 ring-[var(--bronze)]/30":"hover:bg-muted/50"}`,children:(0,b.jsxs)("div",{className:"flex items-center gap-2.5",children:[(0,b.jsx)("div",{className:`w-4 h-4 rounded border-2 flex items-center justify-center transition-all flex-shrink-0 ${c?"border-[var(--bronze)] bg-[var(--bronze)]":"border-muted-foreground/30 group-hover:border-muted-foreground/50"}`,children:c&&(0,b.jsx)(r.Check,{className:"h-2.5 w-2.5 text-background"})}),(0,b.jsxs)("div",{className:"flex-1 min-w-0",children:[(0,b.jsx)("p",{className:`text-sm font-medium ${c?"text-[var(--bronze)]":""}`,children:a.label}),(0,b.jsx)("p",{className:"text-[10px] text-muted-foreground leading-tight",children:a.description})]})]})},a.id)})})})]})})]})}a.s(["default",()=>M],23289)}];

//# sourceMappingURL=app_orchestration_page_tsx_d4dd6b6a._.js.map