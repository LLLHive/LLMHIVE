{"benchmark_id": "swe-bench-verified", "name": "SWE-bench Verified", "category": "software_engineering", "description": "Subset of 500 human-verified GitHub issues from real Python repositories requiring code fixes.", "evaluation_protocol": "Resolved rate - percentage of issues where generated patch passes all tests", "dataset_version": "verified-500", "pitfalls": ["Some issues may have multiple valid solutions not covered by tests", "Python-only limits generalization", "May overfit to common patterns"], "what_it_measures": "End-to-end software engineering: understanding issue, locating code, generating fix, passing tests", "source_url": "https://www.swebench.com/", "retrieved_at": "2024-12-20", "confidence": "high"}
{"benchmark_id": "swe-bench-full", "name": "SWE-bench Full", "category": "software_engineering", "description": "Full 2294 GitHub issues from 12 Python repositories", "evaluation_protocol": "Resolved rate on full dataset", "dataset_version": "full-2294", "pitfalls": ["Many issues are quite easy", "Noisy test suites", "Some issues have incorrect gold patches"], "what_it_measures": "Breadth of software engineering capability across difficulty levels", "source_url": "https://www.swebench.com/", "retrieved_at": "2024-12-20", "confidence": "high"}
{"benchmark_id": "humaneval", "name": "HumanEval", "category": "coding", "description": "164 Python programming problems with function signatures and docstrings", "evaluation_protocol": "pass@k - percentage of problems where at least one of k samples passes tests", "dataset_version": "v1", "pitfalls": ["Simple problems", "Single-function only", "Python-only", "Easily saturated"], "what_it_measures": "Basic function-level code generation from docstrings", "source_url": "https://github.com/openai/human-eval", "retrieved_at": "2024-12-20", "confidence": "high"}
{"benchmark_id": "humaneval-plus", "name": "HumanEval+", "category": "coding", "description": "HumanEval with 80x more tests to catch false positives", "evaluation_protocol": "pass@1 with expanded test suite", "dataset_version": "v0.1.0", "pitfalls": ["Still Python-only", "Function-level only"], "what_it_measures": "More rigorous code correctness vs original HumanEval", "source_url": "https://github.com/evalplus/evalplus", "retrieved_at": "2024-12-20", "confidence": "high"}
{"benchmark_id": "mbpp", "name": "MBPP", "category": "coding", "description": "974 Python programming problems crowd-sourced from workers", "evaluation_protocol": "pass@k on test cases", "dataset_version": "sanitized-426", "pitfalls": ["Variable quality", "Easy problems", "Ambiguous descriptions"], "what_it_measures": "Basic Python coding from natural language descriptions", "source_url": "https://github.com/google-research/google-research/tree/master/mbpp", "retrieved_at": "2024-12-20", "confidence": "high"}
{"benchmark_id": "mmlu", "name": "MMLU", "category": "knowledge", "description": "57 subjects across STEM, humanities, social sciences, and more. 14k multiple choice questions.", "evaluation_protocol": "5-shot accuracy on multiple choice", "dataset_version": "v1", "pitfalls": ["Multiple choice format limits", "Some questions outdated", "Memorization possible"], "what_it_measures": "Breadth of knowledge across academic subjects", "source_url": "https://github.com/hendrycks/test", "retrieved_at": "2024-12-20", "confidence": "high"}
{"benchmark_id": "mmlu-pro", "name": "MMLU-Pro", "category": "knowledge", "description": "Harder version of MMLU with 10 answer choices and more reasoning-focused questions", "evaluation_protocol": "5-shot accuracy with chain-of-thought", "dataset_version": "v1", "pitfalls": ["Still multiple choice"], "what_it_measures": "Knowledge + reasoning over academic subjects", "source_url": "https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro", "retrieved_at": "2024-12-20", "confidence": "high"}
{"benchmark_id": "gsm8k", "name": "GSM8K", "category": "math", "description": "8.5K grade school math word problems requiring multi-step reasoning", "evaluation_protocol": "Exact match on final numerical answer", "dataset_version": "v1", "pitfalls": ["Easy for frontier models", "Single-step extraction can work", "Limited problem variety"], "what_it_measures": "Multi-step arithmetic reasoning with word problems", "source_url": "https://github.com/openai/grade-school-math", "retrieved_at": "2024-12-20", "confidence": "high"}
{"benchmark_id": "math", "name": "MATH", "category": "math", "description": "12.5K competition math problems from AMC, AIME, etc. across 7 difficulty levels", "evaluation_protocol": "Exact match with LaTeX normalization", "dataset_version": "v1", "pitfalls": ["Competition style may not reflect practical math", "LaTeX formatting issues"], "what_it_measures": "Advanced mathematical reasoning and problem solving", "source_url": "https://github.com/hendrycks/math", "retrieved_at": "2024-12-20", "confidence": "high"}
{"benchmark_id": "gpqa", "name": "GPQA Diamond", "category": "reasoning", "description": "448 PhD-level questions in physics, chemistry, biology written by domain experts", "evaluation_protocol": "Multiple choice accuracy", "dataset_version": "diamond", "pitfalls": ["Small dataset", "Some questions have errors"], "what_it_measures": "PhD-level scientific reasoning", "source_url": "https://arxiv.org/abs/2311.12022", "retrieved_at": "2024-12-20", "confidence": "high"}
{"benchmark_id": "arc-challenge", "name": "ARC Challenge", "category": "reasoning", "description": "Hard science questions from standardized tests filtered for difficulty", "evaluation_protocol": "Multiple choice accuracy", "dataset_version": "challenge", "pitfalls": ["Relatively easy now", "Multiple choice format"], "what_it_measures": "Scientific reasoning on grade-school science", "source_url": "https://allenai.org/data/arc", "retrieved_at": "2024-12-20", "confidence": "high"}
{"benchmark_id": "hellaswag", "name": "HellaSwag", "category": "reasoning", "description": "Sentence completion requiring commonsense reasoning", "evaluation_protocol": "Multiple choice accuracy", "dataset_version": "v1", "pitfalls": ["Saturated by current models", "Surface patterns sometimes work"], "what_it_measures": "Commonsense reasoning and text continuation", "source_url": "https://rowanzellers.com/hellaswag/", "retrieved_at": "2024-12-20", "confidence": "high"}
{"benchmark_id": "ifeval", "name": "IFEval", "category": "instruction_following", "description": "Prompts with verifiable constraints like word count, format, inclusion requirements", "evaluation_protocol": "Strict/loose accuracy on constraint satisfaction", "dataset_version": "v1", "pitfalls": ["Artificial constraints", "May not reflect real use"], "what_it_measures": "Precise instruction following and constraint satisfaction", "source_url": "https://arxiv.org/abs/2311.07911", "retrieved_at": "2024-12-20", "confidence": "high"}
{"benchmark_id": "bfcl", "name": "Berkeley Function Calling Leaderboard", "category": "tool_use", "description": "Function calling benchmark testing AST accuracy, relevance detection, multiple calls", "evaluation_protocol": "AST accuracy, hallucination rate, relevance detection", "dataset_version": "v3", "pitfalls": ["Synthetic scenarios", "May not cover real API complexity"], "what_it_measures": "Tool/function calling capability including edge cases", "source_url": "https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html", "retrieved_at": "2024-12-20", "confidence": "high"}
{"benchmark_id": "ruler", "name": "RULER", "category": "long_context", "description": "Long-context benchmark testing needle-in-haystack, multi-hop QA up to 128K tokens", "evaluation_protocol": "Accuracy at various context lengths", "dataset_version": "v1", "pitfalls": ["Synthetic tasks", "May not reflect real use"], "what_it_measures": "Ability to utilize long context effectively", "source_url": "https://arxiv.org/abs/2404.06654", "retrieved_at": "2024-12-20", "confidence": "high"}
{"benchmark_id": "lmsys-arena", "name": "LMSYS Chatbot Arena", "category": "general", "description": "Human preference voting on pairwise model comparisons across diverse prompts", "evaluation_protocol": "Elo rating from blind pairwise votes", "dataset_version": "live", "pitfalls": ["Style bias", "Prompt distribution may not match production", "Gaming possible"], "what_it_measures": "General human preference for model outputs", "source_url": "https://lmarena.ai/", "retrieved_at": "2024-12-20", "confidence": "high"}
{"benchmark_id": "aider-polyglot", "name": "Aider Polyglot", "category": "software_engineering", "description": "Real coding tasks across multiple languages testing agentic code editing", "evaluation_protocol": "Task completion rate with git diff validation", "dataset_version": "v1", "pitfalls": ["Specific to aider-style interaction"], "what_it_measures": "Agentic code editing capability across languages", "source_url": "https://aider.chat/docs/leaderboards/", "retrieved_at": "2024-12-20", "confidence": "medium"}
{"benchmark_id": "tau-bench", "name": "Ï„-bench", "category": "agentic", "description": "Multi-turn agentic benchmark testing tool use over complex retail/airline scenarios", "evaluation_protocol": "Task completion rate over multi-turn interactions", "dataset_version": "v1", "pitfalls": ["Limited domains", "Specific tool schemas"], "what_it_measures": "Multi-turn agentic task completion with tools", "source_url": "https://arxiv.org/abs/2406.12045", "retrieved_at": "2024-12-20", "confidence": "medium"}
{"benchmark_id": "arc-agi", "name": "ARC-AGI", "category": "reasoning", "description": "Abstraction and Reasoning Corpus - visual pattern completion requiring novel reasoning", "evaluation_protocol": "Exact grid match on test examples", "dataset_version": "v1", "pitfalls": ["Very hard - even humans score ~85%", "Requires visual reasoning"], "what_it_measures": "Abstract reasoning and pattern generalization", "source_url": "https://arcprize.org/", "retrieved_at": "2024-12-20", "confidence": "high"}
{"benchmark_id": "simpleqa", "name": "SimpleQA", "category": "factuality", "description": "OpenAI factuality benchmark - simple factual questions with verifiable answers", "evaluation_protocol": "Accuracy on factual claims", "dataset_version": "v1", "pitfalls": ["May favor models trained on similar data"], "what_it_measures": "Factual accuracy on simple questions", "source_url": "https://openai.com/index/introducing-simpleqa/", "retrieved_at": "2024-12-20", "confidence": "high"}

