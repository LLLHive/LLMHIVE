{"model_id": "openai/o1", "benchmark_id": "gpqa", "score": 78.0, "metric": "accuracy", "date_reported": "2024-12-05", "source_url": "https://openai.com/index/deliberative-alignment/", "vendor_reported": true, "notes": "GPQA Diamond subset"}
{"model_id": "openai/o1", "benchmark_id": "math", "score": 94.8, "metric": "accuracy", "date_reported": "2024-12-05", "source_url": "https://openai.com/index/deliberative-alignment/", "vendor_reported": true, "notes": "MATH benchmark"}
{"model_id": "openai/o1", "benchmark_id": "mmlu", "score": 91.8, "metric": "accuracy", "date_reported": "2024-12-05", "source_url": "https://openai.com/index/deliberative-alignment/", "vendor_reported": true, "notes": "MMLU 5-shot"}
{"model_id": "openai/o1", "benchmark_id": "humaneval", "score": 92.4, "metric": "pass@1", "date_reported": "2024-12-05", "source_url": "https://openai.com/index/deliberative-alignment/", "vendor_reported": true, "notes": "HumanEval pass@1"}
{"model_id": "openai/o1", "benchmark_id": "swe-bench-verified", "score": 48.9, "metric": "resolved_rate", "date_reported": "2024-12-05", "source_url": "https://openai.com/index/deliberative-alignment/", "vendor_reported": true, "notes": "SWE-bench Verified"}
{"model_id": "openai/gpt-4o", "benchmark_id": "mmlu", "score": 88.7, "metric": "accuracy", "date_reported": "2024-05-13", "source_url": "https://openai.com/index/hello-gpt-4o/", "vendor_reported": true, "notes": "MMLU 5-shot"}
{"model_id": "openai/gpt-4o", "benchmark_id": "humaneval", "score": 90.2, "metric": "pass@1", "date_reported": "2024-05-13", "source_url": "https://openai.com/index/hello-gpt-4o/", "vendor_reported": true, "notes": "HumanEval pass@1"}
{"model_id": "openai/gpt-4o", "benchmark_id": "math", "score": 76.6, "metric": "accuracy", "date_reported": "2024-05-13", "source_url": "https://openai.com/index/hello-gpt-4o/", "vendor_reported": true, "notes": "MATH benchmark"}
{"model_id": "openai/gpt-4o", "benchmark_id": "swe-bench-verified", "score": 33.2, "metric": "resolved_rate", "date_reported": "2024-10-01", "source_url": "https://www.swebench.com/", "vendor_reported": false, "notes": "SWE-bench Verified independent eval"}
{"model_id": "openai/gpt-4o-mini", "benchmark_id": "mmlu", "score": 82.0, "metric": "accuracy", "date_reported": "2024-07-18", "source_url": "https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/", "vendor_reported": true, "notes": "MMLU 5-shot"}
{"model_id": "openai/gpt-4o-mini", "benchmark_id": "humaneval", "score": 87.0, "metric": "pass@1", "date_reported": "2024-07-18", "source_url": "https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/", "vendor_reported": true, "notes": "HumanEval pass@1"}
{"model_id": "anthropic/claude-3-5-sonnet-20241022", "benchmark_id": "swe-bench-verified", "score": 49.0, "metric": "resolved_rate", "date_reported": "2024-10-22", "source_url": "https://www.anthropic.com/news/claude-3-5-sonnet", "vendor_reported": true, "notes": "SWE-bench Verified with computer use"}
{"model_id": "anthropic/claude-3-5-sonnet-20241022", "benchmark_id": "mmlu", "score": 88.7, "metric": "accuracy", "date_reported": "2024-06-20", "source_url": "https://www.anthropic.com/news/claude-3-5-sonnet", "vendor_reported": true, "notes": "MMLU 5-shot"}
{"model_id": "anthropic/claude-3-5-sonnet-20241022", "benchmark_id": "humaneval", "score": 92.0, "metric": "pass@1", "date_reported": "2024-10-22", "source_url": "https://www.anthropic.com/news/claude-3-5-sonnet", "vendor_reported": true, "notes": "HumanEval pass@1"}
{"model_id": "anthropic/claude-3-5-sonnet-20241022", "benchmark_id": "math", "score": 78.3, "metric": "accuracy", "date_reported": "2024-10-22", "source_url": "https://www.anthropic.com/news/claude-3-5-sonnet", "vendor_reported": true, "notes": "MATH benchmark"}
{"model_id": "anthropic/claude-3-5-sonnet-20241022", "benchmark_id": "gpqa", "score": 65.0, "metric": "accuracy", "date_reported": "2024-10-22", "source_url": "https://www.anthropic.com/news/claude-3-5-sonnet", "vendor_reported": true, "notes": "GPQA Diamond"}
{"model_id": "anthropic/claude-sonnet-4-20250514", "benchmark_id": "swe-bench-verified", "score": 72.7, "metric": "resolved_rate", "date_reported": "2025-05-14", "source_url": "https://www.anthropic.com/news/claude-4", "vendor_reported": true, "notes": "TODO: Verify post-release"}
{"model_id": "anthropic/claude-opus-4-20250514", "benchmark_id": "swe-bench-verified", "score": 72.5, "metric": "resolved_rate", "date_reported": "2025-05-14", "source_url": "https://www.anthropic.com/news/claude-4", "vendor_reported": true, "notes": "TODO: Verify post-release"}
{"model_id": "google/gemini-2.0-flash", "benchmark_id": "mmlu", "score": 85.0, "metric": "accuracy", "date_reported": "2024-12-11", "source_url": "https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/", "vendor_reported": true, "notes": "MMLU 5-shot"}
{"model_id": "google/gemini-2.0-flash", "benchmark_id": "humaneval", "score": 89.0, "metric": "pass@1", "date_reported": "2024-12-11", "source_url": "https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/", "vendor_reported": true, "notes": "HumanEval pass@1"}
{"model_id": "google/gemini-1.5-pro", "benchmark_id": "mmlu", "score": 85.9, "metric": "accuracy", "date_reported": "2024-05-14", "source_url": "https://deepmind.google/technologies/gemini/", "vendor_reported": true, "notes": "MMLU 5-shot"}
{"model_id": "google/gemini-1.5-pro", "benchmark_id": "math", "score": 67.7, "metric": "accuracy", "date_reported": "2024-05-14", "source_url": "https://deepmind.google/technologies/gemini/", "vendor_reported": true, "notes": "MATH benchmark"}
{"model_id": "google/gemini-1.5-pro", "benchmark_id": "ruler", "score": 95.0, "metric": "accuracy_128k", "date_reported": "2024-05-14", "source_url": "https://deepmind.google/technologies/gemini/", "vendor_reported": true, "notes": "RULER at 128K context"}
{"model_id": "meta-llama/llama-3.3-70b-instruct", "benchmark_id": "mmlu", "score": 86.0, "metric": "accuracy", "date_reported": "2024-12-06", "source_url": "https://ai.meta.com/blog/llama-3-3/", "vendor_reported": true, "notes": "MMLU 5-shot"}
{"model_id": "meta-llama/llama-3.3-70b-instruct", "benchmark_id": "humaneval", "score": 88.4, "metric": "pass@1", "date_reported": "2024-12-06", "source_url": "https://ai.meta.com/blog/llama-3-3/", "vendor_reported": true, "notes": "HumanEval pass@1"}
{"model_id": "meta-llama/llama-3.3-70b-instruct", "benchmark_id": "math", "score": 77.0, "metric": "accuracy", "date_reported": "2024-12-06", "source_url": "https://ai.meta.com/blog/llama-3-3/", "vendor_reported": true, "notes": "MATH benchmark"}
{"model_id": "deepseek/deepseek-chat", "benchmark_id": "mmlu", "score": 87.5, "metric": "accuracy", "date_reported": "2024-12-25", "source_url": "https://github.com/deepseek-ai/DeepSeek-V3", "vendor_reported": true, "notes": "DeepSeek V3"}
{"model_id": "deepseek/deepseek-chat", "benchmark_id": "humaneval", "score": 89.3, "metric": "pass@1", "date_reported": "2024-12-25", "source_url": "https://github.com/deepseek-ai/DeepSeek-V3", "vendor_reported": true, "notes": "DeepSeek V3"}
{"model_id": "deepseek/deepseek-chat", "benchmark_id": "math", "score": 75.9, "metric": "accuracy", "date_reported": "2024-12-25", "source_url": "https://github.com/deepseek-ai/DeepSeek-V3", "vendor_reported": true, "notes": "DeepSeek V3"}
{"model_id": "mistralai/mistral-large-2411", "benchmark_id": "mmlu", "score": 84.0, "metric": "accuracy", "date_reported": "2024-11-01", "source_url": "https://mistral.ai/news/mistral-large-2/", "vendor_reported": true, "notes": "MMLU 5-shot"}
{"model_id": "mistralai/codestral-2501", "benchmark_id": "humaneval", "score": 92.5, "metric": "pass@1", "date_reported": "2025-01-01", "source_url": "https://mistral.ai/news/codestral/", "vendor_reported": true, "notes": "TODO: Verify 2501 version"}

